{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9735a4e9-e1fa-4a62-a786-7c827ed5769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Datacenter Computing team (@alexghergh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9456df73-91de-446e-82b8-290cee6dcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a97c09-2026-4c62-ae98-2f38afdecc7d",
   "metadata": {},
   "source": [
    "# Don't worry about not understanding particular/specific things, the most important thing is to get the gist of it, and how the flow works! We'll have plenty of time to practice writing neural networks!\n",
    "\n",
    "# We encourage you to search the Internet for explanations on any topics, as well as using chatbots to explain certain concepts, BUT please try the solutions for yourself first and make sure you understand (at least conceptually) what happens behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9df3d4-2311-4501-b94e-81a3b4dd5401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adf5a7d-d147-467e-b86a-8bb0b6daf13d",
   "metadata": {},
   "source": [
    "## 0. The dataset (MNIST digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95501f73-3725-44bd-adac-57e9002742e0",
   "metadata": {},
   "source": [
    "### The MNIST digits problem consists in classifying a series of images into their corresponding digits classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871e12a-59df-4f8d-867e-9bd1751f3063",
   "metadata": {},
   "source": [
    "It's a well-known problem, introduced in 1998 by Yann LeCun and his colleagues in a paper called \"Gradient-Based Learning Applied to Document Recognition\". It played a significant role in the early 2000s in revining the neural network interest. In contains 60.000 training images and 10.000 test images. More info about the dataset can be found at https://huggingface.co/datasets/ylecun/mnist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102de18-23a5-4cd0-a9e6-92101411692e",
   "metadata": {},
   "source": [
    "We'll download the dataset from the huggingface hub (https://huggingface.co/datasets/ylecun/mnist). The huggingface hub contains a bunch of other datasets, for anything from text to image to audio, so it's a very good resource when looking for data to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80da529-aa17-49fa-9158-28b689e683fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = load_dataset('ylecun/mnist')\n",
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfc4c5-e537-4adb-ae0a-487e41c4c9bf",
   "metadata": {},
   "source": [
    "## 1. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109bd7c-4d16-4760-8f46-eceae58d0a53",
   "metadata": {},
   "source": [
    "Let's try to visualize this dataset and see what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa26989-2f22-46d8-acac-b248eac1f6b3",
   "metadata": {},
   "source": [
    "![MNIST digits problem](./mnist_digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6051a7-693e-458b-a537-287f0a4bd0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+t3w54S1HxN9pltntraytFDXN7eS+VBCDwNzc8n0AJqnrmiXnh/VHsL0RmQKsiPE4dJEYbldWHVSCCKzqK6dtRm1DwdpfhvS7G6aYXMtxdCKIt9oc4EfTJO1QwxjjJrU+KcdzZatomj3isLnS9GtrWYtz8+C5GcnON+PwrhKns7p7K8iuY44ZGiYMEmjWRD7FWBBFey+B/H/iN7TU/E2sauIdD0eIJFp1tDHEk8z58uIKqj5RySevA968k1rXdT8Q6lJf6reS3VxIxOZGJCgnOFH8I9hxWdRUouZxam1E0gt2cSGLcdhYAgNjpnBIz7moqK//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4AWNgGNTAYO2/3VI4XJj9783ue2/VsMo6/vjoxKD1NwmbJEvvFysGBtFvv0SxyLr+LQeJLvsrBpNkgjEYGMIYViE4YBaSJMPeJ0AhRpZrX2FqkCSffuUAiooFX4JLwhTBacu/wXA2BuPpX3O4GAucxaTAwMCXoC31X0roHVwQwuB3Xv33799/QPx+tyFMjhHCyM9VZHi/joHBVe6d+T2YHIxmC64U4WRgYN7x1wUmhEE3/d2DcASarPq/v85oQnAuy9V/3Tg1+v19JQdXCWIgBR+DEkPDIxRJZE7MWWQe7dgAyzQ/yyvH34MAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 148\n",
    "img = mnist['train'][idx]['image']\n",
    "label = mnist['train'][idx]['label']\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941a7e1f-e295-4d6b-af1b-c9a71ab9b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to transform a 28x28 pixel image into a tensor\n",
    "def image_to_tensor(img):\n",
    "    # first we'll use a transform from torchvision\n",
    "    tr = torchvision.transforms.PILToTensor()\n",
    "    img = tr(img)\n",
    "\n",
    "    # we transform to float and divide by 255 (so we work with values between 0 and 1, not 0 and 255)\n",
    "    img = img.float() / 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2339a6d3-b2df-4ceb-95c2-5aed6ffd646d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]),\n",
       " tensor([[[0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.18824,\n",
       "           0.67843, 0.99608, 0.73333, 0.10196, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.41961, 0.99608,\n",
       "           0.92549, 0.73333, 0.87059, 0.92941, 0.14902, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.25490, 0.97255, 0.94510,\n",
       "           0.25882, 0.00000, 0.16471, 0.99216, 0.38431, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.01569, 0.55294, 0.95686, 0.22745,\n",
       "           0.00000, 0.00000, 0.08235, 0.96471, 0.98039, 0.08235, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.27059, 0.99216, 0.46667, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.65098, 0.99216, 0.08627, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.60784, 0.99216, 0.12941, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.65098, 0.99216, 0.08627, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.60784, 0.72941, 0.01961, 0.00000,\n",
       "           0.00000, 0.00000, 0.00392, 0.66667, 0.82745, 0.04314, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.50196, 0.68627, 0.05098, 0.00000,\n",
       "           0.00000, 0.00000, 0.09020, 0.99216, 0.64706, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.22353, 0.99216, 0.32549, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.89804, 0.99216, 0.21569, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00784, 0.13333, 0.13333, 0.13333, 0.18824,\n",
       "           0.56471, 0.73333, 1.00000, 0.99608, 0.31765, 0.07059, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.05882, 0.26275, 0.67059, 0.99216, 0.99216, 0.99216, 0.99608,\n",
       "           0.99216, 0.99216, 0.99216, 0.93725, 0.73333, 0.19216, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.43529,\n",
       "           0.86275, 0.99216, 0.99216, 0.92549, 0.60392, 0.60392, 0.60392,\n",
       "           0.87451, 0.99216, 0.92157, 0.13333, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.02353,\n",
       "           0.32549, 0.47451, 0.07843, 0.03529, 0.00000, 0.00000, 0.01176,\n",
       "           0.72157, 0.99216, 0.26667, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.50980,\n",
       "           0.99216, 0.73725, 0.01569, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.15294, 0.99608,\n",
       "           0.99216, 0.26275, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.01569, 0.83529, 0.99608,\n",
       "           0.54510, 0.01569, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.30588, 0.99216, 0.91765,\n",
       "           0.11765, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.43922, 0.99216, 0.41569,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.36078, 0.80392, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
       "          [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "           0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a tensor of size (1, 28, 28), of float values between 0 and 1\n",
    "img_t = image_to_tensor(img)\n",
    "img_t.shape, img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd7a066-d5e4-4eac-bd2b-46ed64465ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_30e14_row0_col0, #T_30e14_row0_col1, #T_30e14_row0_col2, #T_30e14_row0_col3, #T_30e14_row0_col4, #T_30e14_row0_col5, #T_30e14_row0_col6, #T_30e14_row0_col7, #T_30e14_row0_col8, #T_30e14_row0_col9, #T_30e14_row0_col10, #T_30e14_row0_col11, #T_30e14_row0_col12, #T_30e14_row0_col13, #T_30e14_row0_col14, #T_30e14_row0_col15, #T_30e14_row0_col16, #T_30e14_row0_col17, #T_30e14_row0_col18, #T_30e14_row0_col19, #T_30e14_row0_col20, #T_30e14_row0_col21, #T_30e14_row0_col22, #T_30e14_row0_col23, #T_30e14_row0_col24, #T_30e14_row0_col25, #T_30e14_row0_col26, #T_30e14_row0_col27, #T_30e14_row1_col0, #T_30e14_row1_col1, #T_30e14_row1_col2, #T_30e14_row1_col3, #T_30e14_row1_col4, #T_30e14_row1_col5, #T_30e14_row1_col6, #T_30e14_row1_col7, #T_30e14_row1_col8, #T_30e14_row1_col9, #T_30e14_row1_col10, #T_30e14_row1_col11, #T_30e14_row1_col12, #T_30e14_row1_col13, #T_30e14_row1_col14, #T_30e14_row1_col15, #T_30e14_row1_col16, #T_30e14_row1_col17, #T_30e14_row1_col18, #T_30e14_row1_col19, #T_30e14_row1_col20, #T_30e14_row1_col21, #T_30e14_row1_col22, #T_30e14_row1_col23, #T_30e14_row1_col24, #T_30e14_row1_col25, #T_30e14_row1_col26, #T_30e14_row1_col27, #T_30e14_row2_col0, #T_30e14_row2_col1, #T_30e14_row2_col2, #T_30e14_row2_col3, #T_30e14_row2_col4, #T_30e14_row2_col5, #T_30e14_row2_col6, #T_30e14_row2_col7, #T_30e14_row2_col8, #T_30e14_row2_col9, #T_30e14_row2_col10, #T_30e14_row2_col11, #T_30e14_row2_col12, #T_30e14_row2_col13, #T_30e14_row2_col14, #T_30e14_row2_col15, #T_30e14_row2_col16, #T_30e14_row2_col17, #T_30e14_row2_col18, #T_30e14_row2_col19, #T_30e14_row2_col20, #T_30e14_row2_col21, #T_30e14_row2_col22, #T_30e14_row2_col23, #T_30e14_row2_col24, #T_30e14_row2_col25, #T_30e14_row2_col26, #T_30e14_row2_col27, #T_30e14_row3_col0, #T_30e14_row3_col1, #T_30e14_row3_col2, #T_30e14_row3_col3, #T_30e14_row3_col4, #T_30e14_row3_col5, #T_30e14_row3_col6, #T_30e14_row3_col7, #T_30e14_row3_col8, #T_30e14_row3_col9, #T_30e14_row3_col10, #T_30e14_row3_col11, #T_30e14_row3_col12, #T_30e14_row3_col13, #T_30e14_row3_col14, #T_30e14_row3_col15, #T_30e14_row3_col16, #T_30e14_row3_col17, #T_30e14_row3_col18, #T_30e14_row3_col19, #T_30e14_row3_col20, #T_30e14_row3_col21, #T_30e14_row3_col22, #T_30e14_row3_col23, #T_30e14_row3_col24, #T_30e14_row3_col25, #T_30e14_row3_col26, #T_30e14_row3_col27, #T_30e14_row4_col0, #T_30e14_row4_col1, #T_30e14_row4_col2, #T_30e14_row4_col3, #T_30e14_row4_col4, #T_30e14_row4_col5, #T_30e14_row4_col6, #T_30e14_row4_col7, #T_30e14_row4_col8, #T_30e14_row4_col9, #T_30e14_row4_col10, #T_30e14_row4_col11, #T_30e14_row4_col12, #T_30e14_row4_col13, #T_30e14_row4_col14, #T_30e14_row4_col15, #T_30e14_row4_col16, #T_30e14_row4_col17, #T_30e14_row4_col18, #T_30e14_row4_col19, #T_30e14_row4_col20, #T_30e14_row4_col21, #T_30e14_row4_col22, #T_30e14_row4_col23, #T_30e14_row4_col24, #T_30e14_row4_col25, #T_30e14_row4_col26, #T_30e14_row4_col27, #T_30e14_row5_col0, #T_30e14_row5_col1, #T_30e14_row5_col2, #T_30e14_row5_col3, #T_30e14_row5_col4, #T_30e14_row5_col5, #T_30e14_row5_col6, #T_30e14_row5_col7, #T_30e14_row5_col8, #T_30e14_row5_col9, #T_30e14_row5_col10, #T_30e14_row5_col11, #T_30e14_row5_col12, #T_30e14_row5_col18, #T_30e14_row5_col19, #T_30e14_row5_col20, #T_30e14_row5_col21, #T_30e14_row5_col22, #T_30e14_row5_col23, #T_30e14_row5_col24, #T_30e14_row5_col25, #T_30e14_row5_col26, #T_30e14_row5_col27, #T_30e14_row6_col0, #T_30e14_row6_col1, #T_30e14_row6_col2, #T_30e14_row6_col3, #T_30e14_row6_col4, #T_30e14_row6_col5, #T_30e14_row6_col6, #T_30e14_row6_col7, #T_30e14_row6_col8, #T_30e14_row6_col9, #T_30e14_row6_col10, #T_30e14_row6_col11, #T_30e14_row6_col19, #T_30e14_row6_col20, #T_30e14_row6_col21, #T_30e14_row6_col22, #T_30e14_row6_col23, #T_30e14_row6_col24, #T_30e14_row6_col25, #T_30e14_row6_col26, #T_30e14_row6_col27, #T_30e14_row7_col0, #T_30e14_row7_col1, #T_30e14_row7_col2, #T_30e14_row7_col3, #T_30e14_row7_col4, #T_30e14_row7_col5, #T_30e14_row7_col6, #T_30e14_row7_col7, #T_30e14_row7_col8, #T_30e14_row7_col9, #T_30e14_row7_col10, #T_30e14_row7_col15, #T_30e14_row7_col19, #T_30e14_row7_col20, #T_30e14_row7_col21, #T_30e14_row7_col22, #T_30e14_row7_col23, #T_30e14_row7_col24, #T_30e14_row7_col25, #T_30e14_row7_col26, #T_30e14_row7_col27, #T_30e14_row8_col0, #T_30e14_row8_col1, #T_30e14_row8_col2, #T_30e14_row8_col3, #T_30e14_row8_col4, #T_30e14_row8_col5, #T_30e14_row8_col6, #T_30e14_row8_col7, #T_30e14_row8_col8, #T_30e14_row8_col9, #T_30e14_row8_col14, #T_30e14_row8_col15, #T_30e14_row8_col20, #T_30e14_row8_col21, #T_30e14_row8_col22, #T_30e14_row8_col23, #T_30e14_row8_col24, #T_30e14_row8_col25, #T_30e14_row8_col26, #T_30e14_row8_col27, #T_30e14_row9_col0, #T_30e14_row9_col1, #T_30e14_row9_col2, #T_30e14_row9_col3, #T_30e14_row9_col4, #T_30e14_row9_col5, #T_30e14_row9_col6, #T_30e14_row9_col7, #T_30e14_row9_col8, #T_30e14_row9_col9, #T_30e14_row9_col13, #T_30e14_row9_col14, #T_30e14_row9_col15, #T_30e14_row9_col16, #T_30e14_row9_col20, #T_30e14_row9_col21, #T_30e14_row9_col22, #T_30e14_row9_col23, #T_30e14_row9_col24, #T_30e14_row9_col25, #T_30e14_row9_col26, #T_30e14_row9_col27, #T_30e14_row10_col0, #T_30e14_row10_col1, #T_30e14_row10_col2, #T_30e14_row10_col3, #T_30e14_row10_col4, #T_30e14_row10_col5, #T_30e14_row10_col6, #T_30e14_row10_col7, #T_30e14_row10_col8, #T_30e14_row10_col9, #T_30e14_row10_col13, #T_30e14_row10_col14, #T_30e14_row10_col15, #T_30e14_row10_col16, #T_30e14_row10_col20, #T_30e14_row10_col21, #T_30e14_row10_col22, #T_30e14_row10_col23, #T_30e14_row10_col24, #T_30e14_row10_col25, #T_30e14_row10_col26, #T_30e14_row10_col27, #T_30e14_row11_col0, #T_30e14_row11_col1, #T_30e14_row11_col2, #T_30e14_row11_col3, #T_30e14_row11_col4, #T_30e14_row11_col5, #T_30e14_row11_col6, #T_30e14_row11_col7, #T_30e14_row11_col8, #T_30e14_row11_col9, #T_30e14_row11_col13, #T_30e14_row11_col14, #T_30e14_row11_col15, #T_30e14_row11_col16, #T_30e14_row11_col20, #T_30e14_row11_col21, #T_30e14_row11_col22, #T_30e14_row11_col23, #T_30e14_row11_col24, #T_30e14_row11_col25, #T_30e14_row11_col26, #T_30e14_row11_col27, #T_30e14_row12_col0, #T_30e14_row12_col1, #T_30e14_row12_col2, #T_30e14_row12_col3, #T_30e14_row12_col4, #T_30e14_row12_col5, #T_30e14_row12_col6, #T_30e14_row12_col7, #T_30e14_row12_col8, #T_30e14_row12_col9, #T_30e14_row12_col13, #T_30e14_row12_col14, #T_30e14_row12_col15, #T_30e14_row12_col19, #T_30e14_row12_col20, #T_30e14_row12_col21, #T_30e14_row12_col22, #T_30e14_row12_col23, #T_30e14_row12_col24, #T_30e14_row12_col25, #T_30e14_row12_col26, #T_30e14_row12_col27, #T_30e14_row13_col0, #T_30e14_row13_col1, #T_30e14_row13_col2, #T_30e14_row13_col3, #T_30e14_row13_col4, #T_30e14_row13_col5, #T_30e14_row13_col6, #T_30e14_row13_col7, #T_30e14_row13_col8, #T_30e14_row13_col9, #T_30e14_row13_col10, #T_30e14_row13_col11, #T_30e14_row13_col12, #T_30e14_row13_col13, #T_30e14_row13_col14, #T_30e14_row13_col15, #T_30e14_row13_col19, #T_30e14_row13_col20, #T_30e14_row13_col21, #T_30e14_row13_col22, #T_30e14_row13_col23, #T_30e14_row13_col24, #T_30e14_row13_col25, #T_30e14_row13_col26, #T_30e14_row13_col27, #T_30e14_row14_col0, #T_30e14_row14_col1, #T_30e14_row14_col2, #T_30e14_row14_col3, #T_30e14_row14_col4, #T_30e14_row14_col5, #T_30e14_row14_col6, #T_30e14_row14_col7, #T_30e14_row14_col8, #T_30e14_row14_col9, #T_30e14_row14_col10, #T_30e14_row14_col11, #T_30e14_row14_col12, #T_30e14_row14_col13, #T_30e14_row14_col14, #T_30e14_row14_col15, #T_30e14_row14_col19, #T_30e14_row14_col20, #T_30e14_row14_col21, #T_30e14_row14_col22, #T_30e14_row14_col23, #T_30e14_row14_col24, #T_30e14_row14_col25, #T_30e14_row14_col26, #T_30e14_row14_col27, #T_30e14_row15_col0, #T_30e14_row15_col1, #T_30e14_row15_col2, #T_30e14_row15_col3, #T_30e14_row15_col4, #T_30e14_row15_col5, #T_30e14_row15_col6, #T_30e14_row15_col7, #T_30e14_row15_col8, #T_30e14_row15_col20, #T_30e14_row15_col21, #T_30e14_row15_col22, #T_30e14_row15_col23, #T_30e14_row15_col24, #T_30e14_row15_col25, #T_30e14_row15_col26, #T_30e14_row15_col27, #T_30e14_row16_col0, #T_30e14_row16_col1, #T_30e14_row16_col2, #T_30e14_row16_col3, #T_30e14_row16_col4, #T_30e14_row16_col5, #T_30e14_row16_col6, #T_30e14_row16_col20, #T_30e14_row16_col21, #T_30e14_row16_col22, #T_30e14_row16_col23, #T_30e14_row16_col24, #T_30e14_row16_col25, #T_30e14_row16_col26, #T_30e14_row16_col27, #T_30e14_row17_col0, #T_30e14_row17_col1, #T_30e14_row17_col2, #T_30e14_row17_col3, #T_30e14_row17_col4, #T_30e14_row17_col5, #T_30e14_row17_col18, #T_30e14_row17_col19, #T_30e14_row17_col20, #T_30e14_row17_col21, #T_30e14_row17_col22, #T_30e14_row17_col23, #T_30e14_row17_col24, #T_30e14_row17_col25, #T_30e14_row17_col26, #T_30e14_row17_col27, #T_30e14_row18_col0, #T_30e14_row18_col1, #T_30e14_row18_col2, #T_30e14_row18_col3, #T_30e14_row18_col4, #T_30e14_row18_col5, #T_30e14_row18_col11, #T_30e14_row18_col12, #T_30e14_row18_col17, #T_30e14_row18_col18, #T_30e14_row18_col19, #T_30e14_row18_col20, #T_30e14_row18_col21, #T_30e14_row18_col22, #T_30e14_row18_col23, #T_30e14_row18_col24, #T_30e14_row18_col25, #T_30e14_row18_col26, #T_30e14_row18_col27, #T_30e14_row19_col0, #T_30e14_row19_col1, #T_30e14_row19_col2, #T_30e14_row19_col3, #T_30e14_row19_col4, #T_30e14_row19_col5, #T_30e14_row19_col6, #T_30e14_row19_col7, #T_30e14_row19_col8, #T_30e14_row19_col9, #T_30e14_row19_col10, #T_30e14_row19_col11, #T_30e14_row19_col12, #T_30e14_row19_col17, #T_30e14_row19_col18, #T_30e14_row19_col19, #T_30e14_row19_col20, #T_30e14_row19_col21, #T_30e14_row19_col22, #T_30e14_row19_col23, #T_30e14_row19_col24, #T_30e14_row19_col25, #T_30e14_row19_col26, #T_30e14_row19_col27, #T_30e14_row20_col0, #T_30e14_row20_col1, #T_30e14_row20_col2, #T_30e14_row20_col3, #T_30e14_row20_col4, #T_30e14_row20_col5, #T_30e14_row20_col6, #T_30e14_row20_col7, #T_30e14_row20_col8, #T_30e14_row20_col9, #T_30e14_row20_col10, #T_30e14_row20_col11, #T_30e14_row20_col16, #T_30e14_row20_col17, #T_30e14_row20_col18, #T_30e14_row20_col19, #T_30e14_row20_col20, #T_30e14_row20_col21, #T_30e14_row20_col22, #T_30e14_row20_col23, #T_30e14_row20_col24, #T_30e14_row20_col25, #T_30e14_row20_col26, #T_30e14_row20_col27, #T_30e14_row21_col0, #T_30e14_row21_col1, #T_30e14_row21_col2, #T_30e14_row21_col3, #T_30e14_row21_col4, #T_30e14_row21_col5, #T_30e14_row21_col6, #T_30e14_row21_col7, #T_30e14_row21_col8, #T_30e14_row21_col9, #T_30e14_row21_col10, #T_30e14_row21_col16, #T_30e14_row21_col17, #T_30e14_row21_col18, #T_30e14_row21_col19, #T_30e14_row21_col20, #T_30e14_row21_col21, #T_30e14_row21_col22, #T_30e14_row21_col23, #T_30e14_row21_col24, #T_30e14_row21_col25, #T_30e14_row21_col26, #T_30e14_row21_col27, #T_30e14_row22_col0, #T_30e14_row22_col1, #T_30e14_row22_col2, #T_30e14_row22_col3, #T_30e14_row22_col4, #T_30e14_row22_col5, #T_30e14_row22_col6, #T_30e14_row22_col7, #T_30e14_row22_col8, #T_30e14_row22_col9, #T_30e14_row22_col10, #T_30e14_row22_col15, #T_30e14_row22_col16, #T_30e14_row22_col17, #T_30e14_row22_col18, #T_30e14_row22_col19, #T_30e14_row22_col20, #T_30e14_row22_col21, #T_30e14_row22_col22, #T_30e14_row22_col23, #T_30e14_row22_col24, #T_30e14_row22_col25, #T_30e14_row22_col26, #T_30e14_row22_col27, #T_30e14_row23_col0, #T_30e14_row23_col1, #T_30e14_row23_col2, #T_30e14_row23_col3, #T_30e14_row23_col4, #T_30e14_row23_col5, #T_30e14_row23_col6, #T_30e14_row23_col7, #T_30e14_row23_col8, #T_30e14_row23_col9, #T_30e14_row23_col10, #T_30e14_row23_col14, #T_30e14_row23_col15, #T_30e14_row23_col16, #T_30e14_row23_col17, #T_30e14_row23_col18, #T_30e14_row23_col19, #T_30e14_row23_col20, #T_30e14_row23_col21, #T_30e14_row23_col22, #T_30e14_row23_col23, #T_30e14_row23_col24, #T_30e14_row23_col25, #T_30e14_row23_col26, #T_30e14_row23_col27, #T_30e14_row24_col0, #T_30e14_row24_col1, #T_30e14_row24_col2, #T_30e14_row24_col3, #T_30e14_row24_col4, #T_30e14_row24_col5, #T_30e14_row24_col6, #T_30e14_row24_col7, #T_30e14_row24_col8, #T_30e14_row24_col9, #T_30e14_row24_col10, #T_30e14_row24_col13, #T_30e14_row24_col14, #T_30e14_row24_col15, #T_30e14_row24_col16, #T_30e14_row24_col17, #T_30e14_row24_col18, #T_30e14_row24_col19, #T_30e14_row24_col20, #T_30e14_row24_col21, #T_30e14_row24_col22, #T_30e14_row24_col23, #T_30e14_row24_col24, #T_30e14_row24_col25, #T_30e14_row24_col26, #T_30e14_row24_col27, #T_30e14_row25_col0, #T_30e14_row25_col1, #T_30e14_row25_col2, #T_30e14_row25_col3, #T_30e14_row25_col4, #T_30e14_row25_col5, #T_30e14_row25_col6, #T_30e14_row25_col7, #T_30e14_row25_col8, #T_30e14_row25_col9, #T_30e14_row25_col10, #T_30e14_row25_col11, #T_30e14_row25_col12, #T_30e14_row25_col13, #T_30e14_row25_col14, #T_30e14_row25_col15, #T_30e14_row25_col16, #T_30e14_row25_col17, #T_30e14_row25_col18, #T_30e14_row25_col19, #T_30e14_row25_col20, #T_30e14_row25_col21, #T_30e14_row25_col22, #T_30e14_row25_col23, #T_30e14_row25_col24, #T_30e14_row25_col25, #T_30e14_row25_col26, #T_30e14_row25_col27, #T_30e14_row26_col0, #T_30e14_row26_col1, #T_30e14_row26_col2, #T_30e14_row26_col3, #T_30e14_row26_col4, #T_30e14_row26_col5, #T_30e14_row26_col6, #T_30e14_row26_col7, #T_30e14_row26_col8, #T_30e14_row26_col9, #T_30e14_row26_col10, #T_30e14_row26_col11, #T_30e14_row26_col12, #T_30e14_row26_col13, #T_30e14_row26_col14, #T_30e14_row26_col15, #T_30e14_row26_col16, #T_30e14_row26_col17, #T_30e14_row26_col18, #T_30e14_row26_col19, #T_30e14_row26_col20, #T_30e14_row26_col21, #T_30e14_row26_col22, #T_30e14_row26_col23, #T_30e14_row26_col24, #T_30e14_row26_col25, #T_30e14_row26_col26, #T_30e14_row26_col27, #T_30e14_row27_col0, #T_30e14_row27_col1, #T_30e14_row27_col2, #T_30e14_row27_col3, #T_30e14_row27_col4, #T_30e14_row27_col5, #T_30e14_row27_col6, #T_30e14_row27_col7, #T_30e14_row27_col8, #T_30e14_row27_col9, #T_30e14_row27_col10, #T_30e14_row27_col11, #T_30e14_row27_col12, #T_30e14_row27_col13, #T_30e14_row27_col14, #T_30e14_row27_col15, #T_30e14_row27_col16, #T_30e14_row27_col17, #T_30e14_row27_col18, #T_30e14_row27_col19, #T_30e14_row27_col20, #T_30e14_row27_col21, #T_30e14_row27_col22, #T_30e14_row27_col23, #T_30e14_row27_col24, #T_30e14_row27_col25, #T_30e14_row27_col26, #T_30e14_row27_col27 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ffffff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row5_col13, #T_30e14_row15_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e4e4e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row5_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #636363;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row5_col15, #T_30e14_row6_col13, #T_30e14_row9_col11, #T_30e14_row9_col18, #T_30e14_row10_col11, #T_30e14_row10_col18, #T_30e14_row15_col16, #T_30e14_row15_col17, #T_30e14_row16_col10, #T_30e14_row16_col11, #T_30e14_row16_col12, #T_30e14_row16_col13, #T_30e14_row16_col14, #T_30e14_row16_col19, #T_30e14_row17_col6, #T_30e14_row17_col7, #T_30e14_row17_col8, #T_30e14_row17_col9, #T_30e14_row19_col14, #T_30e14_row20_col13, #T_30e14_row20_col14, #T_30e14_row21_col13, #T_30e14_row22_col12, #T_30e14_row23_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row5_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #565656;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row5_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f3f3f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row6_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #aeaeae;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row6_col14, #T_30e14_row6_col17, #T_30e14_row17_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #141414;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row6_col15, #T_30e14_row11_col11, #T_30e14_row15_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #555555;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row6_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #272727;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row6_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ececec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row7_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d8d8d8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row7_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #060606;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row7_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #0f0f0f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row7_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d7d7d7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row7_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e9e9e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row7_col17, #T_30e14_row12_col17, #T_30e14_row13_col17, #T_30e14_row14_col17, #T_30e14_row16_col15, #T_30e14_row17_col15, #T_30e14_row18_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #010101;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row7_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b9b9b9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row8_col10, #T_30e14_row11_col12, #T_30e14_row19_col16, #T_30e14_row21_col11, #T_30e14_row21_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fdfdfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row8_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #868686;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row8_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #0a0a0a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row8_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #dddddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row8_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f5f5f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row8_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #090909;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row8_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #030303;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row8_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #adadad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row9_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d4d4d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row9_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #9f9f9f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row9_col17, #T_30e14_row10_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #6b6b6b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row9_col19, #T_30e14_row10_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a7a7a7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row10_col10, #T_30e14_row11_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #777777;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row10_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #efefef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row11_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #676767;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row11_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #333333;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row11_col19, #T_30e14_row13_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #dedede;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row12_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #949494;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row12_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #616161;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row12_col12, #T_30e14_row18_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f9f9f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row12_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f4f4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row12_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #6c6c6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row13_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #c8c8c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row14_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1e1e1e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row14_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #dfdfdf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row15_col9, #T_30e14_row18_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fefefe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row15_col10, #T_30e14_row15_col11, #T_30e14_row15_col12, #T_30e14_row17_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #eeeeee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row15_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #838383;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row15_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #cacaca;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row15_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bebebe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row16_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f7f7f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row16_col8, #T_30e14_row20_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d6d6d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row16_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #656565;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row16_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #020202;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row16_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #111111;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row16_col18, #T_30e14_row19_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #545454;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row17_col11, #T_30e14_row17_col12, #T_30e14_row17_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #787878;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row17_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #232323;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row17_col16, #T_30e14_row22_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #171717;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row18_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bdbdbd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row18_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #9d9d9d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row18_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f6f6f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row18_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fbfbfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row18_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #575757;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row18_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d5d5d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row19_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #929292;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row20_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ebebeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row21_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #303030;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row21_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #888888;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row22_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #cccccc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row22_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f1f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row23_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a8a8a8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30e14_row23_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b0b0b0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row24_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bfbfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30e14_row24_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #3c3c3c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_30e14\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_30e14_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_30e14_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_30e14_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_30e14_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_30e14_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_30e14_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_30e14_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_30e14_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_30e14_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_30e14_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_30e14_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "      <th id=\"T_30e14_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n",
       "      <th id=\"T_30e14_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n",
       "      <th id=\"T_30e14_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n",
       "      <th id=\"T_30e14_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n",
       "      <th id=\"T_30e14_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n",
       "      <th id=\"T_30e14_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n",
       "      <th id=\"T_30e14_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n",
       "      <th id=\"T_30e14_level0_col18\" class=\"col_heading level0 col18\" >18</th>\n",
       "      <th id=\"T_30e14_level0_col19\" class=\"col_heading level0 col19\" >19</th>\n",
       "      <th id=\"T_30e14_level0_col20\" class=\"col_heading level0 col20\" >20</th>\n",
       "      <th id=\"T_30e14_level0_col21\" class=\"col_heading level0 col21\" >21</th>\n",
       "      <th id=\"T_30e14_level0_col22\" class=\"col_heading level0 col22\" >22</th>\n",
       "      <th id=\"T_30e14_level0_col23\" class=\"col_heading level0 col23\" >23</th>\n",
       "      <th id=\"T_30e14_level0_col24\" class=\"col_heading level0 col24\" >24</th>\n",
       "      <th id=\"T_30e14_level0_col25\" class=\"col_heading level0 col25\" >25</th>\n",
       "      <th id=\"T_30e14_level0_col26\" class=\"col_heading level0 col26\" >26</th>\n",
       "      <th id=\"T_30e14_level0_col27\" class=\"col_heading level0 col27\" >27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_30e14_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col10\" class=\"data row0 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col11\" class=\"data row0 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col12\" class=\"data row0 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col13\" class=\"data row0 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col14\" class=\"data row0 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col15\" class=\"data row0 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col16\" class=\"data row0 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col17\" class=\"data row0 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col18\" class=\"data row0 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col19\" class=\"data row0 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col20\" class=\"data row0 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col21\" class=\"data row0 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col22\" class=\"data row0 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col23\" class=\"data row0 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col24\" class=\"data row0 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col25\" class=\"data row0 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col26\" class=\"data row0 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row0_col27\" class=\"data row0 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_30e14_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col11\" class=\"data row1 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col12\" class=\"data row1 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col13\" class=\"data row1 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col14\" class=\"data row1 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col15\" class=\"data row1 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col16\" class=\"data row1 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col17\" class=\"data row1 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col18\" class=\"data row1 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col19\" class=\"data row1 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col20\" class=\"data row1 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col21\" class=\"data row1 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col22\" class=\"data row1 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col23\" class=\"data row1 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col24\" class=\"data row1 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col25\" class=\"data row1 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col26\" class=\"data row1 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row1_col27\" class=\"data row1 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_30e14_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col11\" class=\"data row2 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col12\" class=\"data row2 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col13\" class=\"data row2 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col14\" class=\"data row2 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col15\" class=\"data row2 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col16\" class=\"data row2 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col18\" class=\"data row2 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col19\" class=\"data row2 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col20\" class=\"data row2 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col21\" class=\"data row2 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col22\" class=\"data row2 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col23\" class=\"data row2 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col24\" class=\"data row2 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col25\" class=\"data row2 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col26\" class=\"data row2 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row2_col27\" class=\"data row2 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_30e14_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col9\" class=\"data row3 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col10\" class=\"data row3 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col11\" class=\"data row3 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col12\" class=\"data row3 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col13\" class=\"data row3 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col14\" class=\"data row3 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col15\" class=\"data row3 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col16\" class=\"data row3 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col17\" class=\"data row3 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col18\" class=\"data row3 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col19\" class=\"data row3 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col20\" class=\"data row3 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col21\" class=\"data row3 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col22\" class=\"data row3 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col23\" class=\"data row3 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col24\" class=\"data row3 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col25\" class=\"data row3 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col26\" class=\"data row3 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row3_col27\" class=\"data row3 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_30e14_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col8\" class=\"data row4 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col9\" class=\"data row4 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col10\" class=\"data row4 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col11\" class=\"data row4 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col12\" class=\"data row4 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col13\" class=\"data row4 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col14\" class=\"data row4 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col15\" class=\"data row4 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col16\" class=\"data row4 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col17\" class=\"data row4 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col18\" class=\"data row4 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col19\" class=\"data row4 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col20\" class=\"data row4 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col21\" class=\"data row4 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col22\" class=\"data row4 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col23\" class=\"data row4 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col24\" class=\"data row4 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col25\" class=\"data row4 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col26\" class=\"data row4 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row4_col27\" class=\"data row4 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_30e14_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col9\" class=\"data row5 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col10\" class=\"data row5 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col11\" class=\"data row5 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col12\" class=\"data row5 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col13\" class=\"data row5 col13\" >0.188235</td>\n",
       "      <td id=\"T_30e14_row5_col14\" class=\"data row5 col14\" >0.678431</td>\n",
       "      <td id=\"T_30e14_row5_col15\" class=\"data row5 col15\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row5_col16\" class=\"data row5 col16\" >0.733333</td>\n",
       "      <td id=\"T_30e14_row5_col17\" class=\"data row5 col17\" >0.101961</td>\n",
       "      <td id=\"T_30e14_row5_col18\" class=\"data row5 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col19\" class=\"data row5 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col20\" class=\"data row5 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col21\" class=\"data row5 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col22\" class=\"data row5 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col23\" class=\"data row5 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col24\" class=\"data row5 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col25\" class=\"data row5 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col26\" class=\"data row5 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row5_col27\" class=\"data row5 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_30e14_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col9\" class=\"data row6 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col10\" class=\"data row6 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col11\" class=\"data row6 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col12\" class=\"data row6 col12\" >0.419608</td>\n",
       "      <td id=\"T_30e14_row6_col13\" class=\"data row6 col13\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row6_col14\" class=\"data row6 col14\" >0.925490</td>\n",
       "      <td id=\"T_30e14_row6_col15\" class=\"data row6 col15\" >0.733333</td>\n",
       "      <td id=\"T_30e14_row6_col16\" class=\"data row6 col16\" >0.870588</td>\n",
       "      <td id=\"T_30e14_row6_col17\" class=\"data row6 col17\" >0.929412</td>\n",
       "      <td id=\"T_30e14_row6_col18\" class=\"data row6 col18\" >0.149020</td>\n",
       "      <td id=\"T_30e14_row6_col19\" class=\"data row6 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col20\" class=\"data row6 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col21\" class=\"data row6 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col22\" class=\"data row6 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col23\" class=\"data row6 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col24\" class=\"data row6 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col25\" class=\"data row6 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col26\" class=\"data row6 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row6_col27\" class=\"data row6 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_30e14_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col4\" class=\"data row7 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col6\" class=\"data row7 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col9\" class=\"data row7 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col11\" class=\"data row7 col11\" >0.254902</td>\n",
       "      <td id=\"T_30e14_row7_col12\" class=\"data row7 col12\" >0.972549</td>\n",
       "      <td id=\"T_30e14_row7_col13\" class=\"data row7 col13\" >0.945098</td>\n",
       "      <td id=\"T_30e14_row7_col14\" class=\"data row7 col14\" >0.258824</td>\n",
       "      <td id=\"T_30e14_row7_col15\" class=\"data row7 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col16\" class=\"data row7 col16\" >0.164706</td>\n",
       "      <td id=\"T_30e14_row7_col17\" class=\"data row7 col17\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row7_col18\" class=\"data row7 col18\" >0.384314</td>\n",
       "      <td id=\"T_30e14_row7_col19\" class=\"data row7 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col20\" class=\"data row7 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col21\" class=\"data row7 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col22\" class=\"data row7 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col23\" class=\"data row7 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col24\" class=\"data row7 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col25\" class=\"data row7 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col26\" class=\"data row7 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row7_col27\" class=\"data row7 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_30e14_row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col6\" class=\"data row8 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col7\" class=\"data row8 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col8\" class=\"data row8 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col9\" class=\"data row8 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col10\" class=\"data row8 col10\" >0.015686</td>\n",
       "      <td id=\"T_30e14_row8_col11\" class=\"data row8 col11\" >0.552941</td>\n",
       "      <td id=\"T_30e14_row8_col12\" class=\"data row8 col12\" >0.956863</td>\n",
       "      <td id=\"T_30e14_row8_col13\" class=\"data row8 col13\" >0.227451</td>\n",
       "      <td id=\"T_30e14_row8_col14\" class=\"data row8 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col15\" class=\"data row8 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col16\" class=\"data row8 col16\" >0.082353</td>\n",
       "      <td id=\"T_30e14_row8_col17\" class=\"data row8 col17\" >0.964706</td>\n",
       "      <td id=\"T_30e14_row8_col18\" class=\"data row8 col18\" >0.980392</td>\n",
       "      <td id=\"T_30e14_row8_col19\" class=\"data row8 col19\" >0.082353</td>\n",
       "      <td id=\"T_30e14_row8_col20\" class=\"data row8 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col21\" class=\"data row8 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col22\" class=\"data row8 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col23\" class=\"data row8 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col24\" class=\"data row8 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col25\" class=\"data row8 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col26\" class=\"data row8 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row8_col27\" class=\"data row8 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_30e14_row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col6\" class=\"data row9 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col7\" class=\"data row9 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col8\" class=\"data row9 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col9\" class=\"data row9 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col10\" class=\"data row9 col10\" >0.270588</td>\n",
       "      <td id=\"T_30e14_row9_col11\" class=\"data row9 col11\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row9_col12\" class=\"data row9 col12\" >0.466667</td>\n",
       "      <td id=\"T_30e14_row9_col13\" class=\"data row9 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col14\" class=\"data row9 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col15\" class=\"data row9 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col16\" class=\"data row9 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col17\" class=\"data row9 col17\" >0.650980</td>\n",
       "      <td id=\"T_30e14_row9_col18\" class=\"data row9 col18\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row9_col19\" class=\"data row9 col19\" >0.086275</td>\n",
       "      <td id=\"T_30e14_row9_col20\" class=\"data row9 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col21\" class=\"data row9 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col22\" class=\"data row9 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col23\" class=\"data row9 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col24\" class=\"data row9 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col25\" class=\"data row9 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col26\" class=\"data row9 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row9_col27\" class=\"data row9 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_30e14_row10_col0\" class=\"data row10 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col2\" class=\"data row10 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col4\" class=\"data row10 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col6\" class=\"data row10 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col7\" class=\"data row10 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col8\" class=\"data row10 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col9\" class=\"data row10 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col10\" class=\"data row10 col10\" >0.607843</td>\n",
       "      <td id=\"T_30e14_row10_col11\" class=\"data row10 col11\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row10_col12\" class=\"data row10 col12\" >0.129412</td>\n",
       "      <td id=\"T_30e14_row10_col13\" class=\"data row10 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col14\" class=\"data row10 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col15\" class=\"data row10 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col16\" class=\"data row10 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col17\" class=\"data row10 col17\" >0.650980</td>\n",
       "      <td id=\"T_30e14_row10_col18\" class=\"data row10 col18\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row10_col19\" class=\"data row10 col19\" >0.086275</td>\n",
       "      <td id=\"T_30e14_row10_col20\" class=\"data row10 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col21\" class=\"data row10 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col22\" class=\"data row10 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col23\" class=\"data row10 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col24\" class=\"data row10 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col25\" class=\"data row10 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col26\" class=\"data row10 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row10_col27\" class=\"data row10 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_30e14_row11_col0\" class=\"data row11 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col1\" class=\"data row11 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col2\" class=\"data row11 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col4\" class=\"data row11 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col5\" class=\"data row11 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col6\" class=\"data row11 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col7\" class=\"data row11 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col8\" class=\"data row11 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col9\" class=\"data row11 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col10\" class=\"data row11 col10\" >0.607843</td>\n",
       "      <td id=\"T_30e14_row11_col11\" class=\"data row11 col11\" >0.729412</td>\n",
       "      <td id=\"T_30e14_row11_col12\" class=\"data row11 col12\" >0.019608</td>\n",
       "      <td id=\"T_30e14_row11_col13\" class=\"data row11 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col14\" class=\"data row11 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col15\" class=\"data row11 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col16\" class=\"data row11 col16\" >0.003922</td>\n",
       "      <td id=\"T_30e14_row11_col17\" class=\"data row11 col17\" >0.666667</td>\n",
       "      <td id=\"T_30e14_row11_col18\" class=\"data row11 col18\" >0.827451</td>\n",
       "      <td id=\"T_30e14_row11_col19\" class=\"data row11 col19\" >0.043137</td>\n",
       "      <td id=\"T_30e14_row11_col20\" class=\"data row11 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col21\" class=\"data row11 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col22\" class=\"data row11 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col23\" class=\"data row11 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col24\" class=\"data row11 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col25\" class=\"data row11 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col26\" class=\"data row11 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row11_col27\" class=\"data row11 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_30e14_row12_col0\" class=\"data row12 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col1\" class=\"data row12 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col2\" class=\"data row12 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col4\" class=\"data row12 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col5\" class=\"data row12 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col6\" class=\"data row12 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col7\" class=\"data row12 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col8\" class=\"data row12 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col9\" class=\"data row12 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col10\" class=\"data row12 col10\" >0.501961</td>\n",
       "      <td id=\"T_30e14_row12_col11\" class=\"data row12 col11\" >0.686275</td>\n",
       "      <td id=\"T_30e14_row12_col12\" class=\"data row12 col12\" >0.050980</td>\n",
       "      <td id=\"T_30e14_row12_col13\" class=\"data row12 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col14\" class=\"data row12 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col15\" class=\"data row12 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col16\" class=\"data row12 col16\" >0.090196</td>\n",
       "      <td id=\"T_30e14_row12_col17\" class=\"data row12 col17\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row12_col18\" class=\"data row12 col18\" >0.647059</td>\n",
       "      <td id=\"T_30e14_row12_col19\" class=\"data row12 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col20\" class=\"data row12 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col21\" class=\"data row12 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col22\" class=\"data row12 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col23\" class=\"data row12 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col24\" class=\"data row12 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col25\" class=\"data row12 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col26\" class=\"data row12 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row12_col27\" class=\"data row12 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_30e14_row13_col0\" class=\"data row13 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col1\" class=\"data row13 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col2\" class=\"data row13 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col4\" class=\"data row13 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col5\" class=\"data row13 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col6\" class=\"data row13 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col7\" class=\"data row13 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col8\" class=\"data row13 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col9\" class=\"data row13 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col10\" class=\"data row13 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col11\" class=\"data row13 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col12\" class=\"data row13 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col13\" class=\"data row13 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col14\" class=\"data row13 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col15\" class=\"data row13 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col16\" class=\"data row13 col16\" >0.223529</td>\n",
       "      <td id=\"T_30e14_row13_col17\" class=\"data row13 col17\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row13_col18\" class=\"data row13 col18\" >0.325490</td>\n",
       "      <td id=\"T_30e14_row13_col19\" class=\"data row13 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col20\" class=\"data row13 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col21\" class=\"data row13 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col22\" class=\"data row13 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col23\" class=\"data row13 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col24\" class=\"data row13 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col25\" class=\"data row13 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col26\" class=\"data row13 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row13_col27\" class=\"data row13 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_30e14_row14_col0\" class=\"data row14 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col1\" class=\"data row14 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col2\" class=\"data row14 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col4\" class=\"data row14 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col5\" class=\"data row14 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col6\" class=\"data row14 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col7\" class=\"data row14 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col8\" class=\"data row14 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col9\" class=\"data row14 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col10\" class=\"data row14 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col11\" class=\"data row14 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col12\" class=\"data row14 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col13\" class=\"data row14 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col14\" class=\"data row14 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col15\" class=\"data row14 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col16\" class=\"data row14 col16\" >0.898039</td>\n",
       "      <td id=\"T_30e14_row14_col17\" class=\"data row14 col17\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row14_col18\" class=\"data row14 col18\" >0.215686</td>\n",
       "      <td id=\"T_30e14_row14_col19\" class=\"data row14 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col20\" class=\"data row14 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col21\" class=\"data row14 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col22\" class=\"data row14 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col23\" class=\"data row14 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col24\" class=\"data row14 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col25\" class=\"data row14 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col26\" class=\"data row14 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row14_col27\" class=\"data row14 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_30e14_row15_col0\" class=\"data row15 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col1\" class=\"data row15 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col2\" class=\"data row15 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col3\" class=\"data row15 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col4\" class=\"data row15 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col5\" class=\"data row15 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col6\" class=\"data row15 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col7\" class=\"data row15 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col8\" class=\"data row15 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col9\" class=\"data row15 col9\" >0.007843</td>\n",
       "      <td id=\"T_30e14_row15_col10\" class=\"data row15 col10\" >0.133333</td>\n",
       "      <td id=\"T_30e14_row15_col11\" class=\"data row15 col11\" >0.133333</td>\n",
       "      <td id=\"T_30e14_row15_col12\" class=\"data row15 col12\" >0.133333</td>\n",
       "      <td id=\"T_30e14_row15_col13\" class=\"data row15 col13\" >0.188235</td>\n",
       "      <td id=\"T_30e14_row15_col14\" class=\"data row15 col14\" >0.564706</td>\n",
       "      <td id=\"T_30e14_row15_col15\" class=\"data row15 col15\" >0.733333</td>\n",
       "      <td id=\"T_30e14_row15_col16\" class=\"data row15 col16\" >1.000000</td>\n",
       "      <td id=\"T_30e14_row15_col17\" class=\"data row15 col17\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row15_col18\" class=\"data row15 col18\" >0.317647</td>\n",
       "      <td id=\"T_30e14_row15_col19\" class=\"data row15 col19\" >0.070588</td>\n",
       "      <td id=\"T_30e14_row15_col20\" class=\"data row15 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col21\" class=\"data row15 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col22\" class=\"data row15 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col23\" class=\"data row15 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col24\" class=\"data row15 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col25\" class=\"data row15 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col26\" class=\"data row15 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row15_col27\" class=\"data row15 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_30e14_row16_col0\" class=\"data row16 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col1\" class=\"data row16 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col2\" class=\"data row16 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col3\" class=\"data row16 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col4\" class=\"data row16 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col5\" class=\"data row16 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col6\" class=\"data row16 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col7\" class=\"data row16 col7\" >0.058824</td>\n",
       "      <td id=\"T_30e14_row16_col8\" class=\"data row16 col8\" >0.262745</td>\n",
       "      <td id=\"T_30e14_row16_col9\" class=\"data row16 col9\" >0.670588</td>\n",
       "      <td id=\"T_30e14_row16_col10\" class=\"data row16 col10\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col11\" class=\"data row16 col11\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col12\" class=\"data row16 col12\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col13\" class=\"data row16 col13\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row16_col14\" class=\"data row16 col14\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col15\" class=\"data row16 col15\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col16\" class=\"data row16 col16\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row16_col17\" class=\"data row16 col17\" >0.937255</td>\n",
       "      <td id=\"T_30e14_row16_col18\" class=\"data row16 col18\" >0.733333</td>\n",
       "      <td id=\"T_30e14_row16_col19\" class=\"data row16 col19\" >0.192157</td>\n",
       "      <td id=\"T_30e14_row16_col20\" class=\"data row16 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col21\" class=\"data row16 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col22\" class=\"data row16 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col23\" class=\"data row16 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col24\" class=\"data row16 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col25\" class=\"data row16 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col26\" class=\"data row16 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row16_col27\" class=\"data row16 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_30e14_row17_col0\" class=\"data row17 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col1\" class=\"data row17 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col2\" class=\"data row17 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col4\" class=\"data row17 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col5\" class=\"data row17 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col6\" class=\"data row17 col6\" >0.435294</td>\n",
       "      <td id=\"T_30e14_row17_col7\" class=\"data row17 col7\" >0.862745</td>\n",
       "      <td id=\"T_30e14_row17_col8\" class=\"data row17 col8\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row17_col9\" class=\"data row17 col9\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row17_col10\" class=\"data row17 col10\" >0.925490</td>\n",
       "      <td id=\"T_30e14_row17_col11\" class=\"data row17 col11\" >0.603922</td>\n",
       "      <td id=\"T_30e14_row17_col12\" class=\"data row17 col12\" >0.603922</td>\n",
       "      <td id=\"T_30e14_row17_col13\" class=\"data row17 col13\" >0.603922</td>\n",
       "      <td id=\"T_30e14_row17_col14\" class=\"data row17 col14\" >0.874510</td>\n",
       "      <td id=\"T_30e14_row17_col15\" class=\"data row17 col15\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row17_col16\" class=\"data row17 col16\" >0.921569</td>\n",
       "      <td id=\"T_30e14_row17_col17\" class=\"data row17 col17\" >0.133333</td>\n",
       "      <td id=\"T_30e14_row17_col18\" class=\"data row17 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col19\" class=\"data row17 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col20\" class=\"data row17 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col21\" class=\"data row17 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col22\" class=\"data row17 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col23\" class=\"data row17 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col24\" class=\"data row17 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col25\" class=\"data row17 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col26\" class=\"data row17 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row17_col27\" class=\"data row17 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_30e14_row18_col0\" class=\"data row18 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col1\" class=\"data row18 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col2\" class=\"data row18 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col3\" class=\"data row18 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col4\" class=\"data row18 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col5\" class=\"data row18 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col6\" class=\"data row18 col6\" >0.023529</td>\n",
       "      <td id=\"T_30e14_row18_col7\" class=\"data row18 col7\" >0.325490</td>\n",
       "      <td id=\"T_30e14_row18_col8\" class=\"data row18 col8\" >0.474510</td>\n",
       "      <td id=\"T_30e14_row18_col9\" class=\"data row18 col9\" >0.078431</td>\n",
       "      <td id=\"T_30e14_row18_col10\" class=\"data row18 col10\" >0.035294</td>\n",
       "      <td id=\"T_30e14_row18_col11\" class=\"data row18 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col12\" class=\"data row18 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col13\" class=\"data row18 col13\" >0.011765</td>\n",
       "      <td id=\"T_30e14_row18_col14\" class=\"data row18 col14\" >0.721569</td>\n",
       "      <td id=\"T_30e14_row18_col15\" class=\"data row18 col15\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row18_col16\" class=\"data row18 col16\" >0.266667</td>\n",
       "      <td id=\"T_30e14_row18_col17\" class=\"data row18 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col18\" class=\"data row18 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col19\" class=\"data row18 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col20\" class=\"data row18 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col21\" class=\"data row18 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col22\" class=\"data row18 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col23\" class=\"data row18 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col24\" class=\"data row18 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col25\" class=\"data row18 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col26\" class=\"data row18 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row18_col27\" class=\"data row18 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_30e14_row19_col0\" class=\"data row19 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col1\" class=\"data row19 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col2\" class=\"data row19 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col3\" class=\"data row19 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col4\" class=\"data row19 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col5\" class=\"data row19 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col6\" class=\"data row19 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col7\" class=\"data row19 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col8\" class=\"data row19 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col9\" class=\"data row19 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col10\" class=\"data row19 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col11\" class=\"data row19 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col12\" class=\"data row19 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col13\" class=\"data row19 col13\" >0.509804</td>\n",
       "      <td id=\"T_30e14_row19_col14\" class=\"data row19 col14\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row19_col15\" class=\"data row19 col15\" >0.737255</td>\n",
       "      <td id=\"T_30e14_row19_col16\" class=\"data row19 col16\" >0.015686</td>\n",
       "      <td id=\"T_30e14_row19_col17\" class=\"data row19 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col18\" class=\"data row19 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col19\" class=\"data row19 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col20\" class=\"data row19 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col21\" class=\"data row19 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col22\" class=\"data row19 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col23\" class=\"data row19 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col24\" class=\"data row19 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col25\" class=\"data row19 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col26\" class=\"data row19 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row19_col27\" class=\"data row19 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_30e14_row20_col0\" class=\"data row20 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col1\" class=\"data row20 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col2\" class=\"data row20 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col3\" class=\"data row20 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col4\" class=\"data row20 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col5\" class=\"data row20 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col6\" class=\"data row20 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col7\" class=\"data row20 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col8\" class=\"data row20 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col9\" class=\"data row20 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col10\" class=\"data row20 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col11\" class=\"data row20 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col12\" class=\"data row20 col12\" >0.152941</td>\n",
       "      <td id=\"T_30e14_row20_col13\" class=\"data row20 col13\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row20_col14\" class=\"data row20 col14\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row20_col15\" class=\"data row20 col15\" >0.262745</td>\n",
       "      <td id=\"T_30e14_row20_col16\" class=\"data row20 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col17\" class=\"data row20 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col18\" class=\"data row20 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col19\" class=\"data row20 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col20\" class=\"data row20 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col21\" class=\"data row20 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col22\" class=\"data row20 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col23\" class=\"data row20 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col24\" class=\"data row20 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col25\" class=\"data row20 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col26\" class=\"data row20 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row20_col27\" class=\"data row20 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_30e14_row21_col0\" class=\"data row21 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col1\" class=\"data row21 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col2\" class=\"data row21 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col3\" class=\"data row21 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col4\" class=\"data row21 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col5\" class=\"data row21 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col6\" class=\"data row21 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col7\" class=\"data row21 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col8\" class=\"data row21 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col9\" class=\"data row21 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col10\" class=\"data row21 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col11\" class=\"data row21 col11\" >0.015686</td>\n",
       "      <td id=\"T_30e14_row21_col12\" class=\"data row21 col12\" >0.835294</td>\n",
       "      <td id=\"T_30e14_row21_col13\" class=\"data row21 col13\" >0.996078</td>\n",
       "      <td id=\"T_30e14_row21_col14\" class=\"data row21 col14\" >0.545098</td>\n",
       "      <td id=\"T_30e14_row21_col15\" class=\"data row21 col15\" >0.015686</td>\n",
       "      <td id=\"T_30e14_row21_col16\" class=\"data row21 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col17\" class=\"data row21 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col18\" class=\"data row21 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col19\" class=\"data row21 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col20\" class=\"data row21 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col21\" class=\"data row21 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col22\" class=\"data row21 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col23\" class=\"data row21 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col24\" class=\"data row21 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col25\" class=\"data row21 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col26\" class=\"data row21 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row21_col27\" class=\"data row21 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_30e14_row22_col0\" class=\"data row22 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col1\" class=\"data row22 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col2\" class=\"data row22 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col3\" class=\"data row22 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col4\" class=\"data row22 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col5\" class=\"data row22 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col6\" class=\"data row22 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col7\" class=\"data row22 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col8\" class=\"data row22 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col9\" class=\"data row22 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col10\" class=\"data row22 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col11\" class=\"data row22 col11\" >0.305882</td>\n",
       "      <td id=\"T_30e14_row22_col12\" class=\"data row22 col12\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row22_col13\" class=\"data row22 col13\" >0.917647</td>\n",
       "      <td id=\"T_30e14_row22_col14\" class=\"data row22 col14\" >0.117647</td>\n",
       "      <td id=\"T_30e14_row22_col15\" class=\"data row22 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col16\" class=\"data row22 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col17\" class=\"data row22 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col18\" class=\"data row22 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col19\" class=\"data row22 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col20\" class=\"data row22 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col21\" class=\"data row22 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col22\" class=\"data row22 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col23\" class=\"data row22 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col24\" class=\"data row22 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col25\" class=\"data row22 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col26\" class=\"data row22 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row22_col27\" class=\"data row22 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_30e14_row23_col0\" class=\"data row23 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col1\" class=\"data row23 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col2\" class=\"data row23 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col3\" class=\"data row23 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col4\" class=\"data row23 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col5\" class=\"data row23 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col6\" class=\"data row23 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col7\" class=\"data row23 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col8\" class=\"data row23 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col9\" class=\"data row23 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col10\" class=\"data row23 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col11\" class=\"data row23 col11\" >0.439216</td>\n",
       "      <td id=\"T_30e14_row23_col12\" class=\"data row23 col12\" >0.992157</td>\n",
       "      <td id=\"T_30e14_row23_col13\" class=\"data row23 col13\" >0.415686</td>\n",
       "      <td id=\"T_30e14_row23_col14\" class=\"data row23 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col15\" class=\"data row23 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col16\" class=\"data row23 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col17\" class=\"data row23 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col18\" class=\"data row23 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col19\" class=\"data row23 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col20\" class=\"data row23 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col21\" class=\"data row23 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col22\" class=\"data row23 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col23\" class=\"data row23 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col24\" class=\"data row23 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col25\" class=\"data row23 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col26\" class=\"data row23 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row23_col27\" class=\"data row23 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_30e14_row24_col0\" class=\"data row24 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col1\" class=\"data row24 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col2\" class=\"data row24 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col3\" class=\"data row24 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col4\" class=\"data row24 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col5\" class=\"data row24 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col6\" class=\"data row24 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col7\" class=\"data row24 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col8\" class=\"data row24 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col9\" class=\"data row24 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col10\" class=\"data row24 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col11\" class=\"data row24 col11\" >0.360784</td>\n",
       "      <td id=\"T_30e14_row24_col12\" class=\"data row24 col12\" >0.803922</td>\n",
       "      <td id=\"T_30e14_row24_col13\" class=\"data row24 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col14\" class=\"data row24 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col15\" class=\"data row24 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col16\" class=\"data row24 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col17\" class=\"data row24 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col18\" class=\"data row24 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col19\" class=\"data row24 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col20\" class=\"data row24 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col21\" class=\"data row24 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col22\" class=\"data row24 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col23\" class=\"data row24 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col24\" class=\"data row24 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col25\" class=\"data row24 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col26\" class=\"data row24 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row24_col27\" class=\"data row24 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_30e14_row25_col0\" class=\"data row25 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col1\" class=\"data row25 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col2\" class=\"data row25 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col3\" class=\"data row25 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col4\" class=\"data row25 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col5\" class=\"data row25 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col6\" class=\"data row25 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col7\" class=\"data row25 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col8\" class=\"data row25 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col9\" class=\"data row25 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col10\" class=\"data row25 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col11\" class=\"data row25 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col12\" class=\"data row25 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col13\" class=\"data row25 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col14\" class=\"data row25 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col15\" class=\"data row25 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col16\" class=\"data row25 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col17\" class=\"data row25 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col18\" class=\"data row25 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col19\" class=\"data row25 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col20\" class=\"data row25 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col21\" class=\"data row25 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col22\" class=\"data row25 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col23\" class=\"data row25 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col24\" class=\"data row25 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col25\" class=\"data row25 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col26\" class=\"data row25 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row25_col27\" class=\"data row25 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_30e14_row26_col0\" class=\"data row26 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col1\" class=\"data row26 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col2\" class=\"data row26 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col3\" class=\"data row26 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col4\" class=\"data row26 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col5\" class=\"data row26 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col6\" class=\"data row26 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col7\" class=\"data row26 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col8\" class=\"data row26 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col9\" class=\"data row26 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col10\" class=\"data row26 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col11\" class=\"data row26 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col12\" class=\"data row26 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col13\" class=\"data row26 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col14\" class=\"data row26 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col15\" class=\"data row26 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col16\" class=\"data row26 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col17\" class=\"data row26 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col18\" class=\"data row26 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col19\" class=\"data row26 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col20\" class=\"data row26 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col21\" class=\"data row26 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col22\" class=\"data row26 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col23\" class=\"data row26 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col24\" class=\"data row26 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col25\" class=\"data row26 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col26\" class=\"data row26 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row26_col27\" class=\"data row26 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30e14_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_30e14_row27_col0\" class=\"data row27 col0\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col1\" class=\"data row27 col1\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col2\" class=\"data row27 col2\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col3\" class=\"data row27 col3\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col4\" class=\"data row27 col4\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col5\" class=\"data row27 col5\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col6\" class=\"data row27 col6\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col7\" class=\"data row27 col7\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col8\" class=\"data row27 col8\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col9\" class=\"data row27 col9\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col10\" class=\"data row27 col10\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col11\" class=\"data row27 col11\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col12\" class=\"data row27 col12\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col13\" class=\"data row27 col13\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col14\" class=\"data row27 col14\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col15\" class=\"data row27 col15\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col16\" class=\"data row27 col16\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col17\" class=\"data row27 col17\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col18\" class=\"data row27 col18\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col19\" class=\"data row27 col19\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col20\" class=\"data row27 col20\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col21\" class=\"data row27 col21\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col22\" class=\"data row27 col22\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col23\" class=\"data row27 col23\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col24\" class=\"data row27 col24\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col25\" class=\"data row27 col25\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col26\" class=\"data row27 col26\" >0.000000</td>\n",
       "      <td id=\"T_30e14_row27_col27\" class=\"data row27 col27\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3b6f9a9f10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize this tensor image more neatly\n",
    "df = pd.DataFrame(img_t[0])\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc99d7-7bb2-46fc-97e5-dedd958360c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01259d9-3a95-4bc7-8647-d0feda169b7f",
   "metadata": {},
   "source": [
    "## 2. The neural network model and the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf2591-4558-4fed-8913-de1a62e4414c",
   "metadata": {},
   "source": [
    "We will use the PyTorch framework in order to build a neural network. The simplest neural network we can have is simply a stack between linear functions and non-linear (ReLU = Rectified Linear Unit) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd4f44-aa2a-445f-9421-debbed8d48ed",
   "metadata": {},
   "source": [
    "What we'll do in order to be able to pass an image through this network is to linearize the 28x28 tensor image, in a 784-entries array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4484b8-5af3-490b-abdb-a3cc46529946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), torch.Size([1, 784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the docs: https://pytorch.org/docs/stable/generated/torch.reshape.html\n",
    "img_t_resized = img_t.reshape(shape=(1,28*28))\n",
    "img_t.shape, img_t_resized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d27e5-3477-4d09-b16a-0855658e0b4f",
   "metadata": {},
   "source": [
    "Now let's create a model to pass this image through. Notice that our output (the dimension of linear layer) is 10, because we have 10 categories an image can be in (the digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afadd8cf-9db2-49af-a323-ed199e5553b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28, 10),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9af0b-ec70-4d02-a115-c51a2a8e0245",
   "metadata": {},
   "source": [
    "Let's print the total number of parameters, which, for a linear a*x+b, is equal to the numberof elements in the matrix a (size 784x10), plus the number of elements in the matrix b (size 1x10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f950ffd7-9e8c-416e-916d-46e434c83f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1d3e3-3a09-44bc-816c-9a02fa1c7471",
   "metadata": {},
   "source": [
    "Now we need a loss (error) function, to tell us how far off we are from the actual values. The way this will work is as follows:\n",
    "1. Have the model output the 10 values, each value corresponding to each of the 10 digits.\n",
    "2. Extract the value with the highest probability, and consider that as the model prediction (i.e. if the 4th value has the highest probability out of the 10, then we consider the model output to be \"3\", because we start at 0).\n",
    "3. Compare this with the label of the image (e.g. an image with a label of \"3\" would ideally have a model output of [0, 0, 0, 1, 0, 0, 0, 0, 0, 0])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640908f7-42f6-4e69-a335-d002bea14948",
   "metadata": {},
   "source": [
    "This sort of loss is called Cross-Entropy loss. We need this, as opposed to MSE (mean squared error), because we are working with categories now, not simply continuous values.\n",
    "\n",
    "Read the PyTorch documentation to be sure what this does: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2926ac63-30a0-4aa2-93f3-f55d41b75114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660e890-c58c-4dc1-9ae1-e9f7b4934b1e",
   "metadata": {},
   "source": [
    "We also need an \"optimizer\", i.e. a way to update (optimize) the parameters of the neural networks. This is the piece of code that automatically computes the new values for parameters, based on their gradients:\n",
    "```python\n",
    "a = a - lr * a_grad\n",
    "```\n",
    "\n",
    "For now, we'll use a very simple optimizer, the Stochastic Gradient Descent (SGD in short) optimizer. There are newer and better optimizers, but this is simple enough that we can use it and get something meaningful.\n",
    "\n",
    "Make sure to read the docs to understand more about it: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html.\n",
    "\n",
    "We'll use a learning rate of 0.005. Too big or small of a learning rate will make the model diverge, or train very very slowly, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb71357-c0a9-4c1a-9a6d-e8cdc7be89e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.005\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533f441-f898-49a0-b361-cd34e7b4679f",
   "metadata": {},
   "source": [
    "Finally, we need something to serve us the data. In PyTorch, this is called a DataLoader. See the docs at https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n",
    "\n",
    "We'll use our MNIST dataset as the training dataset, with a batch size of 64 (i.e. we'll use 64 images through the model at one time). The loss function knows how to average those 64 images at one time. In practice, this makes the model more \"stable\" (converges to a solution faster/easier). There's a trade-off here, as with everything in engineering. Too big of a batch size, and the model might diverge, similar with a too small of a batch size. Usually values between 32 and 512 are safe to use with images.\n",
    "\n",
    "The batch size and learning rate are the 2 most important hyperparameters one needs to set. You can try multiple combinations and see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6868800-66ef-4ce0-be86-92240734684b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f3b6c3075d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to transform our data a bit before passing to the DataLoader, because\n",
    "# a DataLoader only accepts tensors, but we have images; we'll use the same transformation\n",
    "# from above, but this time on the whole dataset\n",
    "def preprocess_data(example):\n",
    "    for i in range(len(example['image'])):\n",
    "        # convert image to tensor\n",
    "        example['image'][i] = image_to_tensor(example['image'][i])\n",
    "        # reshape into an array/vector, instead of a matrix\n",
    "        example['image'][i] = example['image'][i].reshape(shape=(1, 28*28))\n",
    "    return example\n",
    "\n",
    "# preprocess the dataset\n",
    "mnist = mnist.with_transform(preprocess_data)\n",
    "\n",
    "# create the dataloader (we'll also shuffle the images, as it helps the model see images in random order)\n",
    "dl = torch.utils.data.DataLoader(mnist['train'], batch_size=64, shuffle=True)\n",
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f5d10-65e7-4be6-8034-a00316ed232b",
   "metadata": {},
   "source": [
    "We finally have our training loop. This is the \"heart\" of neural network training, where the magic happens. We'll use our buildings blocks from above in order to train our model.\n",
    "\n",
    "Remember the batch size and learning rate are the most important hyperparameters of a model. The third most important is arguably the number of epochs, which is the number of iterations through the whole dataset. 3 epochs means the model will see each image from the dataset 3 times, so it will have a chance to \"learn\" better about it. Usually, setting the epochs too high will cause the model to \"overfit\" on this dataset (i.e. it will generalize poorly to other, unseen images) so we don't want to set this too high. A number of epochs between 1 and 10 is generally fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1559beaf-9763-40aa-8de5-a6021925ee8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.358027219772339\n",
      "loss:  2.348088502883911\n",
      "loss:  2.313857078552246\n",
      "loss:  2.287343740463257\n",
      "loss:  2.327712297439575\n",
      "loss:  2.281404972076416\n",
      "loss:  2.296492576599121\n",
      "loss:  2.2960920333862305\n",
      "loss:  2.2570159435272217\n",
      "loss:  2.2767181396484375\n",
      "loss:  2.2999255657196045\n",
      "loss:  2.2911107540130615\n",
      "loss:  2.2863337993621826\n",
      "loss:  2.2461414337158203\n",
      "loss:  2.237787961959839\n",
      "loss:  2.251513957977295\n",
      "loss:  2.252425193786621\n",
      "loss:  2.249817132949829\n",
      "loss:  2.2403218746185303\n",
      "loss:  2.2218704223632812\n",
      "loss:  2.2092981338500977\n",
      "loss:  2.217432975769043\n",
      "loss:  2.1876332759857178\n",
      "loss:  2.236436128616333\n",
      "loss:  2.1774168014526367\n",
      "loss:  2.205493927001953\n",
      "loss:  2.202807903289795\n",
      "loss:  2.2033252716064453\n",
      "loss:  2.199788808822632\n",
      "loss:  2.144768238067627\n",
      "loss:  2.1658873558044434\n",
      "loss:  2.1339876651763916\n",
      "loss:  2.1697163581848145\n",
      "loss:  2.164271831512451\n",
      "loss:  2.144907236099243\n",
      "loss:  2.131092071533203\n",
      "loss:  2.1581459045410156\n",
      "loss:  2.2104368209838867\n",
      "loss:  2.133784770965576\n",
      "loss:  2.1362392902374268\n",
      "loss:  2.1301259994506836\n",
      "loss:  2.133206605911255\n",
      "loss:  2.111562728881836\n",
      "loss:  2.1046717166900635\n",
      "loss:  2.121307134628296\n",
      "loss:  2.066007614135742\n",
      "loss:  2.091775894165039\n",
      "loss:  2.0704824924468994\n",
      "loss:  2.0748307704925537\n",
      "loss:  2.032667398452759\n",
      "loss:  2.0897061824798584\n",
      "loss:  2.1381635665893555\n",
      "loss:  2.0330700874328613\n",
      "loss:  2.078723192214966\n",
      "loss:  2.0070173740386963\n",
      "loss:  2.0926876068115234\n",
      "loss:  2.0530383586883545\n",
      "loss:  2.055337429046631\n",
      "loss:  2.03694224357605\n",
      "loss:  2.029855966567993\n",
      "loss:  2.028791666030884\n",
      "loss:  2.0445938110351562\n",
      "loss:  2.024491310119629\n",
      "loss:  2.0730433464050293\n",
      "loss:  2.0616750717163086\n",
      "loss:  2.001065254211426\n",
      "loss:  2.03950834274292\n",
      "loss:  1.995853304862976\n",
      "loss:  2.057004451751709\n",
      "loss:  2.0044872760772705\n",
      "loss:  1.9905837774276733\n",
      "loss:  2.034313201904297\n",
      "loss:  2.0246620178222656\n",
      "loss:  1.9764567613601685\n",
      "loss:  1.9724501371383667\n",
      "loss:  1.9388984441757202\n",
      "loss:  1.9284164905548096\n",
      "loss:  1.958519458770752\n",
      "loss:  1.9361838102340698\n",
      "loss:  1.9871066808700562\n",
      "loss:  1.99650239944458\n",
      "loss:  1.965014100074768\n",
      "loss:  1.9066976308822632\n",
      "loss:  1.9189649820327759\n",
      "loss:  1.9078325033187866\n",
      "loss:  1.938852310180664\n",
      "loss:  1.9390485286712646\n",
      "loss:  1.8542124032974243\n",
      "loss:  1.877112865447998\n",
      "loss:  1.940093994140625\n",
      "loss:  1.976486325263977\n",
      "loss:  1.8831064701080322\n",
      "loss:  1.9183660745620728\n",
      "loss:  1.9065659046173096\n",
      "loss:  1.8447253704071045\n",
      "loss:  1.8642703294754028\n",
      "loss:  1.8042765855789185\n",
      "loss:  1.8772674798965454\n",
      "loss:  1.8394300937652588\n",
      "loss:  1.9269421100616455\n",
      "loss:  1.870995044708252\n",
      "loss:  1.8811300992965698\n",
      "loss:  1.7858455181121826\n",
      "loss:  1.8851990699768066\n",
      "loss:  1.8814760446548462\n",
      "loss:  1.8824944496154785\n",
      "loss:  1.8927335739135742\n",
      "loss:  1.875968098640442\n",
      "loss:  1.8538308143615723\n",
      "loss:  1.8220946788787842\n",
      "loss:  1.81055748462677\n",
      "loss:  1.8823195695877075\n",
      "loss:  1.8197919130325317\n",
      "loss:  1.7946608066558838\n",
      "loss:  1.8107486963272095\n",
      "loss:  1.7971742153167725\n",
      "loss:  1.8519201278686523\n",
      "loss:  1.863844871520996\n",
      "loss:  1.8135416507720947\n",
      "loss:  1.819270372390747\n",
      "loss:  1.8259251117706299\n",
      "loss:  1.842403531074524\n",
      "loss:  1.7867467403411865\n",
      "loss:  1.758325457572937\n",
      "loss:  1.8054300546646118\n",
      "loss:  1.8535265922546387\n",
      "loss:  1.759676218032837\n",
      "loss:  1.7578277587890625\n",
      "loss:  1.7518202066421509\n",
      "loss:  1.7199106216430664\n",
      "loss:  1.7723336219787598\n",
      "loss:  1.7448636293411255\n",
      "loss:  1.8104608058929443\n",
      "loss:  1.7800838947296143\n",
      "loss:  1.6824902296066284\n",
      "loss:  1.7812970876693726\n",
      "loss:  1.723923683166504\n",
      "loss:  1.6740775108337402\n",
      "loss:  1.7282027006149292\n",
      "loss:  1.681847333908081\n",
      "loss:  1.662717342376709\n",
      "loss:  1.77577543258667\n",
      "loss:  1.7759807109832764\n",
      "loss:  1.7731446027755737\n",
      "loss:  1.7456225156784058\n",
      "loss:  1.7622013092041016\n",
      "loss:  1.70233154296875\n",
      "loss:  1.6446040868759155\n",
      "loss:  1.6773567199707031\n",
      "loss:  1.6841721534729004\n",
      "loss:  1.6510581970214844\n",
      "loss:  1.6604697704315186\n",
      "loss:  1.761735200881958\n",
      "loss:  1.684152603149414\n",
      "loss:  1.756873369216919\n",
      "loss:  1.6934688091278076\n",
      "loss:  1.641064167022705\n",
      "loss:  1.7443145513534546\n",
      "loss:  1.6172897815704346\n",
      "loss:  1.6712924242019653\n",
      "loss:  1.7383450269699097\n",
      "loss:  1.6119706630706787\n",
      "loss:  1.6821389198303223\n",
      "loss:  1.6984466314315796\n",
      "loss:  1.6665546894073486\n",
      "loss:  1.5918794870376587\n",
      "loss:  1.5782333612442017\n",
      "loss:  1.6806292533874512\n",
      "loss:  1.727982997894287\n",
      "loss:  1.6746848821640015\n",
      "loss:  1.5882081985473633\n",
      "loss:  1.6743026971817017\n",
      "loss:  1.6745554208755493\n",
      "loss:  1.672006607055664\n",
      "loss:  1.6708792448043823\n",
      "loss:  1.6190191507339478\n",
      "loss:  1.626016616821289\n",
      "loss:  1.6247774362564087\n",
      "loss:  1.5573498010635376\n",
      "loss:  1.6808300018310547\n",
      "loss:  1.571418046951294\n",
      "loss:  1.6223851442337036\n",
      "loss:  1.6108781099319458\n",
      "loss:  1.5920472145080566\n",
      "loss:  1.685265302658081\n",
      "loss:  1.5314855575561523\n",
      "loss:  1.5635671615600586\n",
      "loss:  1.5715686082839966\n",
      "loss:  1.5460224151611328\n",
      "loss:  1.6605403423309326\n",
      "loss:  1.6268601417541504\n",
      "loss:  1.5714571475982666\n",
      "loss:  1.5412516593933105\n",
      "loss:  1.6315772533416748\n",
      "loss:  1.6372451782226562\n",
      "loss:  1.6017272472381592\n",
      "loss:  1.611334204673767\n",
      "loss:  1.5461952686309814\n",
      "loss:  1.5224584341049194\n",
      "loss:  1.522405743598938\n",
      "loss:  1.5725891590118408\n",
      "loss:  1.520906925201416\n",
      "loss:  1.5223582983016968\n",
      "loss:  1.5828187465667725\n",
      "loss:  1.5240228176116943\n",
      "loss:  1.568142294883728\n",
      "loss:  1.4680081605911255\n",
      "loss:  1.4166303873062134\n",
      "loss:  1.5533561706542969\n",
      "loss:  1.4897971153259277\n",
      "loss:  1.5777263641357422\n",
      "loss:  1.4745908975601196\n",
      "loss:  1.5907222032546997\n",
      "loss:  1.5041112899780273\n",
      "loss:  1.543073058128357\n",
      "loss:  1.5516141653060913\n",
      "loss:  1.5479645729064941\n",
      "loss:  1.6406294107437134\n",
      "loss:  1.6532211303710938\n",
      "loss:  1.4592703580856323\n",
      "loss:  1.5008914470672607\n",
      "loss:  1.542357087135315\n",
      "loss:  1.5518739223480225\n",
      "loss:  1.4473059177398682\n",
      "loss:  1.383514165878296\n",
      "loss:  1.443849802017212\n",
      "loss:  1.4822434186935425\n",
      "loss:  1.370635986328125\n",
      "loss:  1.499151349067688\n",
      "loss:  1.4663091897964478\n",
      "loss:  1.5803583860397339\n",
      "loss:  1.5216494798660278\n",
      "loss:  1.4828964471817017\n",
      "loss:  1.4892284870147705\n",
      "loss:  1.4542462825775146\n",
      "loss:  1.4150019884109497\n",
      "loss:  1.5337897539138794\n",
      "loss:  1.6096014976501465\n",
      "loss:  1.614694595336914\n",
      "loss:  1.4044848680496216\n",
      "loss:  1.4116294384002686\n",
      "loss:  1.4498294591903687\n",
      "loss:  1.417341947555542\n",
      "loss:  1.4529303312301636\n",
      "loss:  1.4015355110168457\n",
      "loss:  1.350456714630127\n",
      "loss:  1.5549756288528442\n",
      "loss:  1.4857977628707886\n",
      "loss:  1.4880092144012451\n",
      "loss:  1.4693756103515625\n",
      "loss:  1.4497650861740112\n",
      "loss:  1.4348267316818237\n",
      "loss:  1.4386733770370483\n",
      "loss:  1.4264529943466187\n",
      "loss:  1.464131236076355\n",
      "loss:  1.4636718034744263\n",
      "loss:  1.3935270309448242\n",
      "loss:  1.3662315607070923\n",
      "loss:  1.3591543436050415\n",
      "loss:  1.44318425655365\n",
      "loss:  1.517296552658081\n",
      "loss:  1.408089518547058\n",
      "loss:  1.341666579246521\n",
      "loss:  1.3257262706756592\n",
      "loss:  1.366590142250061\n",
      "loss:  1.4412013292312622\n",
      "loss:  1.4385058879852295\n",
      "loss:  1.4348366260528564\n",
      "loss:  1.4252643585205078\n",
      "loss:  1.3466827869415283\n",
      "loss:  1.3696240186691284\n",
      "loss:  1.5044550895690918\n",
      "loss:  1.41507089138031\n",
      "loss:  1.3922464847564697\n",
      "loss:  1.49553382396698\n",
      "loss:  1.3000017404556274\n",
      "loss:  1.4357415437698364\n",
      "loss:  1.5376774072647095\n",
      "loss:  1.4634524583816528\n",
      "loss:  1.34084153175354\n",
      "loss:  1.3802623748779297\n",
      "loss:  1.4410501718521118\n",
      "loss:  1.3461805582046509\n",
      "loss:  1.3363343477249146\n",
      "loss:  1.4579848051071167\n",
      "loss:  1.3490508794784546\n",
      "loss:  1.3362282514572144\n",
      "loss:  1.3135552406311035\n",
      "loss:  1.3969449996948242\n",
      "loss:  1.3804340362548828\n",
      "loss:  1.3578695058822632\n",
      "loss:  1.3726876974105835\n",
      "loss:  1.3866081237792969\n",
      "loss:  1.2680526971817017\n",
      "loss:  1.345428228378296\n",
      "loss:  1.3185042142868042\n",
      "loss:  1.3169935941696167\n",
      "loss:  1.284077763557434\n",
      "loss:  1.2835808992385864\n",
      "loss:  1.2943958044052124\n",
      "loss:  1.2929925918579102\n",
      "loss:  1.3517504930496216\n",
      "loss:  1.303254246711731\n",
      "loss:  1.280121088027954\n",
      "loss:  1.4474663734436035\n",
      "loss:  1.265067458152771\n",
      "loss:  1.2156020402908325\n",
      "loss:  1.395872950553894\n",
      "loss:  1.3184945583343506\n",
      "loss:  1.447866678237915\n",
      "loss:  1.4017164707183838\n",
      "loss:  1.3636701107025146\n",
      "loss:  1.2491170167922974\n",
      "loss:  1.3144046068191528\n",
      "loss:  1.3163965940475464\n",
      "loss:  1.2856318950653076\n",
      "loss:  1.3852646350860596\n",
      "loss:  1.3999645709991455\n",
      "loss:  1.2849938869476318\n",
      "loss:  1.324942946434021\n",
      "loss:  1.2007777690887451\n",
      "loss:  1.3485537767410278\n",
      "loss:  1.310453176498413\n",
      "loss:  1.2749801874160767\n",
      "loss:  1.3468724489212036\n",
      "loss:  1.4483680725097656\n",
      "loss:  1.315826416015625\n",
      "loss:  1.365614891052246\n",
      "loss:  1.336500883102417\n",
      "loss:  1.2989271879196167\n",
      "loss:  1.393926739692688\n",
      "loss:  1.2941229343414307\n",
      "loss:  1.2416452169418335\n",
      "loss:  1.376531720161438\n",
      "loss:  1.3065550327301025\n",
      "loss:  1.2999368906021118\n",
      "loss:  1.2104988098144531\n",
      "loss:  1.2560795545578003\n",
      "loss:  1.2135330438613892\n",
      "loss:  1.4293138980865479\n",
      "loss:  1.2949448823928833\n",
      "loss:  1.3144285678863525\n",
      "loss:  1.2725622653961182\n",
      "loss:  1.3065096139907837\n",
      "loss:  1.211220383644104\n",
      "loss:  1.2594467401504517\n",
      "loss:  1.4065608978271484\n",
      "loss:  1.2392617464065552\n",
      "loss:  1.342311978340149\n",
      "loss:  1.307668685913086\n",
      "loss:  1.1928504705429077\n",
      "loss:  1.2501578330993652\n",
      "loss:  1.194111704826355\n",
      "loss:  1.340495228767395\n",
      "loss:  1.275015115737915\n",
      "loss:  1.3500804901123047\n",
      "loss:  1.2786641120910645\n",
      "loss:  1.3053048849105835\n",
      "loss:  1.0995995998382568\n",
      "loss:  1.1465563774108887\n",
      "loss:  1.2639573812484741\n",
      "loss:  1.2696105241775513\n",
      "loss:  1.2019844055175781\n",
      "loss:  1.2571790218353271\n",
      "loss:  1.132650375366211\n",
      "loss:  1.195530652999878\n",
      "loss:  1.3251919746398926\n",
      "loss:  1.192979335784912\n",
      "loss:  1.2463566064834595\n",
      "loss:  1.2407513856887817\n",
      "loss:  1.2751610279083252\n",
      "loss:  1.3039747476577759\n",
      "loss:  1.1785999536514282\n",
      "loss:  1.2685372829437256\n",
      "loss:  1.2454476356506348\n",
      "loss:  1.1914292573928833\n",
      "loss:  1.191071629524231\n",
      "loss:  1.0804494619369507\n",
      "loss:  1.211178183555603\n",
      "loss:  1.1465214490890503\n",
      "loss:  1.2649987936019897\n",
      "loss:  1.1868935823440552\n",
      "loss:  1.2912545204162598\n",
      "loss:  1.275813341140747\n",
      "loss:  1.0911561250686646\n",
      "loss:  1.2748138904571533\n",
      "loss:  1.314009189605713\n",
      "loss:  1.1914873123168945\n",
      "loss:  1.1825827360153198\n",
      "loss:  1.2236416339874268\n",
      "loss:  1.183099627494812\n",
      "loss:  0.9867295026779175\n",
      "loss:  1.1226917505264282\n",
      "loss:  1.3313242197036743\n",
      "loss:  1.2489978075027466\n",
      "loss:  1.2327393293380737\n",
      "loss:  1.166716456413269\n",
      "loss:  1.2200437784194946\n",
      "loss:  1.1602572202682495\n",
      "loss:  1.214990496635437\n",
      "loss:  1.186347246170044\n",
      "loss:  1.2973277568817139\n",
      "loss:  1.2152891159057617\n",
      "loss:  1.3353668451309204\n",
      "loss:  1.230367660522461\n",
      "loss:  1.2279338836669922\n",
      "loss:  1.1822426319122314\n",
      "loss:  1.2596008777618408\n",
      "loss:  1.12428617477417\n",
      "loss:  1.0724670886993408\n",
      "loss:  1.2464691400527954\n",
      "loss:  1.2342385053634644\n",
      "loss:  1.1620830297470093\n",
      "loss:  1.1944971084594727\n",
      "loss:  1.0757955312728882\n",
      "loss:  1.2223913669586182\n",
      "loss:  1.137213945388794\n",
      "loss:  1.0635465383529663\n",
      "loss:  1.1656889915466309\n",
      "loss:  1.13327956199646\n",
      "loss:  1.3431206941604614\n",
      "loss:  1.09457266330719\n",
      "loss:  1.1420986652374268\n",
      "loss:  1.1467541456222534\n",
      "loss:  1.2690322399139404\n",
      "loss:  1.208141565322876\n",
      "loss:  1.1506404876708984\n",
      "loss:  1.0618436336517334\n",
      "loss:  1.1560091972351074\n",
      "loss:  1.2091944217681885\n",
      "loss:  1.084633708000183\n",
      "loss:  1.1562529802322388\n",
      "loss:  1.2298496961593628\n",
      "loss:  1.1113624572753906\n",
      "loss:  1.0880846977233887\n",
      "loss:  1.1336299180984497\n",
      "loss:  1.0605409145355225\n",
      "loss:  1.1211315393447876\n",
      "loss:  1.1359257698059082\n",
      "loss:  1.0499136447906494\n",
      "loss:  1.0144553184509277\n",
      "loss:  1.2133734226226807\n",
      "loss:  1.1643208265304565\n",
      "loss:  1.2984113693237305\n",
      "loss:  1.0955921411514282\n",
      "loss:  1.1742089986801147\n",
      "loss:  1.0972031354904175\n",
      "loss:  1.0554313659667969\n",
      "loss:  1.0905171632766724\n",
      "loss:  1.2274218797683716\n",
      "loss:  1.2632890939712524\n",
      "loss:  1.1565221548080444\n",
      "loss:  1.2514803409576416\n",
      "loss:  1.08805251121521\n",
      "loss:  1.0789812803268433\n",
      "loss:  1.033601999282837\n",
      "loss:  1.1773666143417358\n",
      "loss:  1.1081569194793701\n",
      "loss:  1.130628228187561\n",
      "loss:  1.1975101232528687\n",
      "loss:  1.1969940662384033\n",
      "loss:  1.1203677654266357\n",
      "loss:  1.1297310590744019\n",
      "loss:  1.0335890054702759\n",
      "loss:  1.1814024448394775\n",
      "loss:  0.9315162897109985\n",
      "loss:  1.0538208484649658\n",
      "loss:  1.1110093593597412\n",
      "loss:  1.1073031425476074\n",
      "loss:  1.1178650856018066\n",
      "loss:  1.2246825695037842\n",
      "loss:  1.2520800828933716\n",
      "loss:  1.1633378267288208\n",
      "loss:  1.1022111177444458\n",
      "loss:  1.2256096601486206\n",
      "loss:  1.1076698303222656\n",
      "loss:  1.0531505346298218\n",
      "loss:  1.1473453044891357\n",
      "loss:  1.0696200132369995\n",
      "loss:  1.082238793373108\n",
      "loss:  1.1432075500488281\n",
      "loss:  1.0445510149002075\n",
      "loss:  1.1248846054077148\n",
      "loss:  1.1070069074630737\n",
      "loss:  1.0244202613830566\n",
      "loss:  1.0617247819900513\n",
      "loss:  1.0995006561279297\n",
      "loss:  1.1061334609985352\n",
      "loss:  1.1725808382034302\n",
      "loss:  1.075806736946106\n",
      "loss:  1.015179991722107\n",
      "loss:  1.1444743871688843\n",
      "loss:  1.1875965595245361\n",
      "loss:  0.9681367874145508\n",
      "loss:  1.029931664466858\n",
      "loss:  0.9959498643875122\n",
      "loss:  1.1858705282211304\n",
      "loss:  1.003408432006836\n",
      "loss:  1.11580228805542\n",
      "loss:  1.0815649032592773\n",
      "loss:  1.0919920206069946\n",
      "loss:  1.122933030128479\n",
      "loss:  1.152463436126709\n",
      "loss:  0.9856876730918884\n",
      "loss:  1.1493570804595947\n",
      "loss:  1.1706275939941406\n",
      "loss:  1.057152271270752\n",
      "loss:  1.0842825174331665\n",
      "loss:  1.0274494886398315\n",
      "loss:  1.188565969467163\n",
      "loss:  1.1433234214782715\n",
      "loss:  0.9993202686309814\n",
      "loss:  1.1274000406265259\n",
      "loss:  0.9947448968887329\n",
      "loss:  1.057563304901123\n",
      "loss:  1.1547861099243164\n",
      "loss:  1.061954140663147\n",
      "loss:  1.0724753141403198\n",
      "loss:  1.2061725854873657\n",
      "loss:  1.132360577583313\n",
      "loss:  1.0276830196380615\n",
      "loss:  1.0704755783081055\n",
      "loss:  1.1223134994506836\n",
      "loss:  1.1988621950149536\n",
      "loss:  1.0563697814941406\n",
      "loss:  0.9947627186775208\n",
      "loss:  1.0328712463378906\n",
      "loss:  1.1712040901184082\n",
      "loss:  1.0662603378295898\n",
      "loss:  0.935739278793335\n",
      "loss:  1.1464506387710571\n",
      "loss:  0.9977055788040161\n",
      "loss:  1.1305360794067383\n",
      "loss:  1.1440072059631348\n",
      "loss:  1.0681384801864624\n",
      "loss:  1.0957361459732056\n",
      "loss:  1.0765193700790405\n",
      "loss:  1.0989205837249756\n",
      "loss:  1.0045950412750244\n",
      "loss:  1.0145618915557861\n",
      "loss:  0.9861934185028076\n",
      "loss:  0.9349862337112427\n",
      "loss:  1.0286884307861328\n",
      "loss:  1.0642720460891724\n",
      "loss:  0.9771823287010193\n",
      "loss:  1.1078211069107056\n",
      "loss:  1.0861105918884277\n",
      "loss:  1.1164922714233398\n",
      "loss:  0.9171894192695618\n",
      "loss:  1.004318118095398\n",
      "loss:  1.1776000261306763\n",
      "loss:  1.0252238512039185\n",
      "loss:  1.0051250457763672\n",
      "loss:  1.0487053394317627\n",
      "loss:  1.1066744327545166\n",
      "loss:  0.9791821837425232\n",
      "loss:  0.9847649931907654\n",
      "loss:  1.0929070711135864\n",
      "loss:  1.0349804162979126\n",
      "loss:  1.0652892589569092\n",
      "loss:  1.0376938581466675\n",
      "loss:  1.148297667503357\n",
      "loss:  0.9595586061477661\n",
      "loss:  0.9214847087860107\n",
      "loss:  0.9739016890525818\n",
      "loss:  1.0693589448928833\n",
      "loss:  1.0089476108551025\n",
      "loss:  0.892719030380249\n",
      "loss:  0.9703540205955505\n",
      "loss:  1.0162880420684814\n",
      "loss:  0.9985471367835999\n",
      "loss:  1.1346572637557983\n",
      "loss:  1.0234850645065308\n",
      "loss:  1.01932692527771\n",
      "loss:  1.076282262802124\n",
      "loss:  0.9704082608222961\n",
      "loss:  1.084365725517273\n",
      "loss:  0.9547553658485413\n",
      "loss:  0.9678338766098022\n",
      "loss:  1.0543440580368042\n",
      "loss:  0.9800231456756592\n",
      "loss:  0.9580407738685608\n",
      "loss:  1.0217845439910889\n",
      "loss:  0.911298930644989\n",
      "loss:  1.2429438829421997\n",
      "loss:  0.9974827766418457\n",
      "loss:  0.9931035041809082\n",
      "loss:  1.0709800720214844\n",
      "loss:  0.9545348286628723\n",
      "loss:  0.9543446898460388\n",
      "loss:  1.0347079038619995\n",
      "loss:  1.1144487857818604\n",
      "loss:  1.038637638092041\n",
      "loss:  0.9539143443107605\n",
      "loss:  1.077153205871582\n",
      "loss:  1.0278892517089844\n",
      "loss:  1.0510369539260864\n",
      "loss:  0.9825009703636169\n",
      "loss:  1.0178685188293457\n",
      "loss:  1.0812236070632935\n",
      "loss:  1.0464149713516235\n",
      "loss:  1.0708544254302979\n",
      "loss:  1.0122579336166382\n",
      "loss:  0.9858656525611877\n",
      "loss:  1.0173059701919556\n",
      "loss:  1.1007404327392578\n",
      "loss:  1.019553780555725\n",
      "loss:  1.2405295372009277\n",
      "loss:  0.9692959189414978\n",
      "loss:  0.952456533908844\n",
      "loss:  0.9404973983764648\n",
      "loss:  1.1098681688308716\n",
      "loss:  1.0051523447036743\n",
      "loss:  0.9828230142593384\n",
      "loss:  0.9919979572296143\n",
      "loss:  0.8741458058357239\n",
      "loss:  1.0082660913467407\n",
      "loss:  1.0113235712051392\n",
      "loss:  1.1137365102767944\n",
      "loss:  0.9973549246788025\n",
      "loss:  0.9188138246536255\n",
      "loss:  0.848712146282196\n",
      "loss:  1.093119740486145\n",
      "loss:  1.0745680332183838\n",
      "loss:  0.9935794472694397\n",
      "loss:  0.8827268481254578\n",
      "loss:  1.129913330078125\n",
      "loss:  0.9903016090393066\n",
      "loss:  1.0117357969284058\n",
      "loss:  0.9938745498657227\n",
      "loss:  0.9001955986022949\n",
      "loss:  1.0507841110229492\n",
      "loss:  0.989068329334259\n",
      "loss:  0.9991936087608337\n",
      "loss:  0.9364917278289795\n",
      "loss:  0.9676285982131958\n",
      "loss:  0.8965286612510681\n",
      "loss:  0.981630265712738\n",
      "loss:  0.9556640982627869\n",
      "loss:  0.9591626524925232\n",
      "loss:  1.0038812160491943\n",
      "loss:  0.9369587302207947\n",
      "loss:  1.0734935998916626\n",
      "loss:  0.9156728982925415\n",
      "loss:  0.8942769169807434\n",
      "loss:  1.0211100578308105\n",
      "loss:  0.9754610061645508\n",
      "loss:  1.0332586765289307\n",
      "loss:  0.9515910744667053\n",
      "loss:  1.025941252708435\n",
      "loss:  0.9566645622253418\n",
      "loss:  1.0447567701339722\n",
      "loss:  0.9652215242385864\n",
      "loss:  0.9205886721611023\n",
      "loss:  0.834580659866333\n",
      "loss:  0.9794304370880127\n",
      "loss:  0.9656916260719299\n",
      "loss:  1.0363924503326416\n",
      "loss:  0.8681502938270569\n",
      "loss:  1.0361518859863281\n",
      "loss:  0.9979747533798218\n",
      "loss:  0.9987446665763855\n",
      "loss:  1.0153350830078125\n",
      "loss:  0.9068975448608398\n",
      "loss:  1.0359127521514893\n",
      "loss:  0.8480652570724487\n",
      "loss:  0.9129884839057922\n",
      "loss:  0.9736981987953186\n",
      "loss:  0.929253339767456\n",
      "loss:  0.9864985942840576\n",
      "loss:  1.023093819618225\n",
      "loss:  1.0007636547088623\n",
      "loss:  0.9556137323379517\n",
      "loss:  1.038946270942688\n",
      "loss:  0.966974139213562\n",
      "loss:  0.9553536176681519\n",
      "loss:  0.952792763710022\n",
      "loss:  0.8386088013648987\n",
      "loss:  0.8585407137870789\n",
      "loss:  1.0180718898773193\n",
      "loss:  1.0133132934570312\n",
      "loss:  1.0715837478637695\n",
      "loss:  0.9190865755081177\n",
      "loss:  0.9408779144287109\n",
      "loss:  1.0151859521865845\n",
      "loss:  1.0040229558944702\n",
      "loss:  0.964996874332428\n",
      "loss:  0.8985757827758789\n",
      "loss:  0.9357457160949707\n",
      "loss:  0.9721431136131287\n",
      "loss:  0.8506711721420288\n",
      "loss:  0.9103482365608215\n",
      "loss:  0.9977908730506897\n",
      "loss:  0.9969887137413025\n",
      "loss:  0.9127503633499146\n",
      "loss:  0.9624768495559692\n",
      "loss:  0.9225262403488159\n",
      "loss:  0.9909252524375916\n",
      "loss:  0.9639801383018494\n",
      "loss:  0.8422591090202332\n",
      "loss:  0.7828080058097839\n",
      "loss:  0.8974369764328003\n",
      "loss:  1.0088328123092651\n",
      "loss:  0.9878478050231934\n",
      "loss:  0.9219273328781128\n",
      "loss:  0.9584850668907166\n",
      "loss:  0.9592430591583252\n",
      "loss:  0.8373985290527344\n",
      "loss:  1.0040831565856934\n",
      "loss:  0.8423635363578796\n",
      "loss:  0.8934054970741272\n",
      "loss:  1.0027258396148682\n",
      "loss:  1.084780216217041\n",
      "loss:  0.8993476033210754\n",
      "loss:  0.9457329511642456\n",
      "loss:  0.9723632335662842\n",
      "loss:  0.8436140418052673\n",
      "loss:  0.8156946301460266\n",
      "loss:  1.0180572271347046\n",
      "loss:  0.9150124788284302\n",
      "loss:  0.8755004405975342\n",
      "loss:  0.9701485633850098\n",
      "loss:  0.9974234700202942\n",
      "loss:  0.8514196276664734\n",
      "loss:  0.9325245022773743\n",
      "loss:  0.9159566164016724\n",
      "loss:  0.9563020467758179\n",
      "loss:  0.9711416959762573\n",
      "loss:  0.9941810965538025\n",
      "loss:  0.8591880798339844\n",
      "loss:  0.7834795117378235\n",
      "loss:  0.7953088283538818\n",
      "loss:  0.9329518675804138\n",
      "loss:  0.7924634218215942\n",
      "loss:  0.7782852649688721\n",
      "loss:  0.9541675448417664\n",
      "loss:  0.8256987929344177\n",
      "loss:  0.9600282311439514\n",
      "loss:  1.0075409412384033\n",
      "loss:  0.8683347105979919\n",
      "loss:  0.9715996384620667\n",
      "loss:  0.9030464887619019\n",
      "loss:  1.0033962726593018\n",
      "loss:  0.9474828839302063\n",
      "loss:  0.9355181455612183\n",
      "loss:  0.9114032983779907\n",
      "loss:  0.952721118927002\n",
      "loss:  0.8161854147911072\n",
      "loss:  0.9398056268692017\n",
      "loss:  0.9038462042808533\n",
      "loss:  0.9555817246437073\n",
      "loss:  0.86573326587677\n",
      "loss:  0.7754890322685242\n",
      "loss:  0.865487813949585\n",
      "loss:  0.9139406681060791\n",
      "loss:  0.8572227358818054\n",
      "loss:  0.8075980544090271\n",
      "loss:  0.9198428988456726\n",
      "loss:  1.0170485973358154\n",
      "loss:  0.9566383957862854\n",
      "loss:  0.9417068362236023\n",
      "loss:  0.9984776377677917\n",
      "loss:  1.0178465843200684\n",
      "loss:  0.8941237926483154\n",
      "loss:  0.9092664122581482\n",
      "loss:  0.8129523396492004\n",
      "loss:  0.9352295398712158\n",
      "loss:  1.0138887166976929\n",
      "loss:  1.00735342502594\n",
      "loss:  0.8175073862075806\n",
      "loss:  0.8419570922851562\n",
      "loss:  0.916115939617157\n",
      "loss:  1.0040993690490723\n",
      "loss:  0.9187021255493164\n",
      "loss:  0.8365580439567566\n",
      "loss:  0.9518821835517883\n",
      "loss:  0.8577810525894165\n",
      "loss:  0.9471865296363831\n",
      "loss:  0.9542670249938965\n",
      "loss:  0.9303925633430481\n",
      "loss:  0.9752876162528992\n",
      "loss:  0.9868835210800171\n",
      "loss:  0.8500950932502747\n",
      "loss:  0.9073172211647034\n",
      "loss:  0.9649065732955933\n",
      "loss:  0.8368808627128601\n",
      "loss:  1.0026503801345825\n",
      "loss:  0.7538068890571594\n",
      "loss:  0.9083871841430664\n",
      "loss:  0.8138902187347412\n",
      "loss:  0.9001293182373047\n",
      "loss:  0.9504218697547913\n",
      "loss:  0.9156807661056519\n",
      "loss:  0.7309545278549194\n",
      "loss:  0.8729290962219238\n",
      "loss:  0.9270828366279602\n",
      "loss:  0.8131351470947266\n",
      "loss:  0.9129691123962402\n",
      "loss:  0.8853927850723267\n",
      "loss:  0.8932279348373413\n",
      "loss:  0.9809433221817017\n",
      "loss:  0.8838924169540405\n",
      "loss:  0.8526272177696228\n",
      "loss:  0.9384081363677979\n",
      "loss:  0.8787045478820801\n",
      "loss:  1.0249632596969604\n",
      "loss:  0.9420776963233948\n",
      "loss:  0.9603370428085327\n",
      "loss:  0.8013823628425598\n",
      "loss:  0.8796852827072144\n",
      "loss:  0.8381842374801636\n",
      "loss:  0.9170037508010864\n",
      "loss:  0.8171443939208984\n",
      "loss:  0.96929931640625\n",
      "loss:  0.9446902275085449\n",
      "loss:  0.6951165199279785\n",
      "loss:  0.8686286807060242\n",
      "loss:  0.8995616436004639\n",
      "loss:  0.7874269485473633\n",
      "loss:  0.7186516523361206\n",
      "loss:  0.8989407420158386\n",
      "loss:  0.9658536314964294\n",
      "loss:  0.8583306670188904\n",
      "loss:  1.1402724981307983\n",
      "loss:  0.9088230729103088\n",
      "loss:  1.005733609199524\n",
      "loss:  0.9112659692764282\n",
      "loss:  0.9860590696334839\n",
      "loss:  0.9293818473815918\n",
      "loss:  0.7057188153266907\n",
      "loss:  0.9292023181915283\n",
      "loss:  1.026521921157837\n",
      "loss:  0.9259044528007507\n",
      "loss:  0.8662143349647522\n",
      "loss:  0.9424597024917603\n",
      "loss:  0.8251860737800598\n",
      "loss:  0.8334192633628845\n",
      "loss:  0.9107455611228943\n",
      "loss:  0.8438779711723328\n",
      "loss:  0.9829140305519104\n",
      "loss:  0.8309348821640015\n",
      "loss:  0.8754529356956482\n",
      "loss:  0.9478882551193237\n",
      "loss:  0.8591082692146301\n",
      "loss:  0.8633798360824585\n",
      "loss:  0.7632720470428467\n",
      "loss:  0.9551953673362732\n",
      "loss:  0.8431762456893921\n",
      "loss:  0.9003633856773376\n",
      "loss:  0.7379623055458069\n",
      "loss:  0.9230803847312927\n",
      "loss:  0.9293670654296875\n",
      "loss:  0.9066531658172607\n",
      "loss:  0.9079715609550476\n",
      "loss:  0.7728589773178101\n",
      "loss:  0.9024178981781006\n",
      "loss:  0.9310835599899292\n",
      "loss:  0.8653727769851685\n",
      "loss:  0.8672544360160828\n",
      "loss:  0.8808368444442749\n",
      "loss:  0.8666335344314575\n",
      "loss:  0.7640672922134399\n",
      "loss:  0.8274028301239014\n",
      "loss:  0.948278546333313\n",
      "loss:  0.9036886096000671\n",
      "loss:  0.9157474040985107\n",
      "loss:  0.824408769607544\n",
      "loss:  0.8519193530082703\n",
      "loss:  0.8604704737663269\n",
      "loss:  0.9678064584732056\n",
      "loss:  0.760258138179779\n",
      "loss:  0.7655708193778992\n",
      "loss:  0.7857759594917297\n",
      "loss:  0.8586495518684387\n",
      "loss:  0.8808929920196533\n",
      "loss:  0.7910537719726562\n",
      "loss:  0.7586110830307007\n",
      "loss:  0.7859583497047424\n",
      "loss:  0.990799069404602\n",
      "loss:  0.9658509492874146\n",
      "loss:  0.8126987218856812\n",
      "loss:  0.7480664253234863\n",
      "loss:  0.8109521865844727\n",
      "loss:  0.9385247230529785\n",
      "loss:  0.8421764373779297\n",
      "loss:  1.058428168296814\n",
      "loss:  0.7245567440986633\n",
      "loss:  0.9448440074920654\n",
      "loss:  0.8814629316329956\n",
      "loss:  0.7749475836753845\n",
      "loss:  0.8264968395233154\n",
      "loss:  0.8209682106971741\n",
      "loss:  0.8260192275047302\n",
      "loss:  0.7768800258636475\n",
      "loss:  0.8903706073760986\n",
      "loss:  0.7934067845344543\n",
      "loss:  0.9007276892662048\n",
      "loss:  0.8463814854621887\n",
      "loss:  0.7510825991630554\n",
      "loss:  0.7788218259811401\n",
      "loss:  0.8492312431335449\n",
      "loss:  0.937815248966217\n",
      "loss:  0.7795618772506714\n",
      "loss:  0.8826326727867126\n",
      "loss:  0.7857397794723511\n",
      "loss:  0.9473282694816589\n",
      "loss:  1.0228593349456787\n",
      "loss:  0.8413141965866089\n",
      "loss:  0.7884684205055237\n",
      "loss:  0.9334032535552979\n",
      "loss:  0.8983327150344849\n",
      "loss:  0.7422183752059937\n",
      "loss:  0.8425654172897339\n",
      "loss:  0.7985788583755493\n",
      "loss:  0.6786565780639648\n",
      "loss:  0.771664023399353\n",
      "loss:  0.766637921333313\n",
      "loss:  0.8832495212554932\n",
      "loss:  0.9629697799682617\n",
      "loss:  0.8916462063789368\n",
      "loss:  0.8119549751281738\n",
      "loss:  0.913314938545227\n",
      "loss:  0.8711957931518555\n",
      "loss:  0.8191805481910706\n",
      "loss:  0.7850745916366577\n",
      "loss:  0.8829158544540405\n",
      "loss:  0.7537806034088135\n",
      "loss:  0.7845293879508972\n",
      "loss:  0.7706859707832336\n",
      "loss:  0.9667456746101379\n",
      "loss:  0.691834032535553\n",
      "loss:  0.7319290041923523\n",
      "loss:  0.9428227543830872\n",
      "loss:  0.7980997562408447\n",
      "loss:  0.96549391746521\n",
      "loss:  0.8619095683097839\n",
      "loss:  0.7763728499412537\n",
      "loss:  0.7057931423187256\n",
      "loss:  0.818406343460083\n",
      "loss:  0.8614609837532043\n",
      "loss:  0.7424392700195312\n",
      "loss:  0.8081212639808655\n",
      "loss:  0.7495208978652954\n",
      "loss:  0.9586480259895325\n",
      "loss:  0.9248632192611694\n",
      "loss:  0.722830057144165\n",
      "loss:  0.8726904988288879\n",
      "loss:  0.8109023571014404\n",
      "loss:  0.8320050239562988\n",
      "loss:  0.8702520132064819\n",
      "loss:  0.8664187788963318\n",
      "loss:  0.7345570921897888\n",
      "loss:  0.8394831418991089\n",
      "loss:  0.8042522668838501\n",
      "loss:  0.7024710178375244\n",
      "loss:  0.8560023307800293\n",
      "loss:  0.7897201776504517\n",
      "loss:  0.7455654144287109\n",
      "loss:  0.9093508720397949\n",
      "loss:  0.7911733388900757\n",
      "loss:  0.8008118867874146\n",
      "loss:  0.7717233896255493\n",
      "loss:  0.7445544004440308\n",
      "loss:  0.7061299085617065\n",
      "loss:  0.6965017914772034\n",
      "loss:  0.9255822896957397\n",
      "loss:  0.8069343566894531\n",
      "loss:  0.8227946758270264\n",
      "loss:  0.7116479277610779\n",
      "loss:  0.71092289686203\n",
      "loss:  0.8156155943870544\n",
      "loss:  0.7473455667495728\n",
      "loss:  0.8451673984527588\n",
      "loss:  0.6111781597137451\n",
      "loss:  0.7350965738296509\n",
      "loss:  0.8346611261367798\n",
      "loss:  0.9886636137962341\n",
      "loss:  0.7759095430374146\n",
      "loss:  0.812102735042572\n",
      "loss:  0.8162663578987122\n",
      "loss:  0.7634644508361816\n",
      "loss:  0.8256049752235413\n",
      "loss:  0.842200756072998\n",
      "loss:  0.7829163074493408\n",
      "loss:  0.8455303907394409\n",
      "loss:  0.7995249032974243\n",
      "loss:  0.8627220392227173\n",
      "loss:  0.7999300360679626\n",
      "loss:  0.7931328415870667\n",
      "loss:  0.8079390525817871\n",
      "loss:  0.8313597440719604\n",
      "loss:  0.7718105912208557\n",
      "loss:  0.9039955139160156\n",
      "loss:  0.7514263987541199\n",
      "loss:  0.7889437675476074\n",
      "loss:  0.8222507238388062\n",
      "loss:  0.7575700283050537\n",
      "loss:  0.8533593416213989\n",
      "loss:  0.8377252221107483\n",
      "loss:  0.8266822695732117\n",
      "loss:  0.747378945350647\n",
      "loss:  0.8120852708816528\n",
      "loss:  0.8081788420677185\n",
      "loss:  0.8185917735099792\n",
      "loss:  0.8566950559616089\n",
      "loss:  0.8150335550308228\n",
      "loss:  0.7807142734527588\n",
      "loss:  0.7720494270324707\n",
      "loss:  0.8530439138412476\n",
      "loss:  0.836301863193512\n",
      "loss:  0.8128239512443542\n",
      "loss:  0.7656384110450745\n",
      "loss:  0.9525490403175354\n",
      "loss:  0.8111406564712524\n",
      "loss:  0.7748636603355408\n",
      "loss:  0.7966738343238831\n",
      "loss:  0.8307474851608276\n",
      "loss:  0.9304447770118713\n",
      "loss:  0.838458240032196\n",
      "loss:  0.7297365069389343\n",
      "loss:  0.9909317493438721\n",
      "loss:  0.7730675339698792\n",
      "loss:  0.6794612407684326\n",
      "loss:  0.8064008951187134\n",
      "loss:  0.8317415714263916\n",
      "loss:  0.994185209274292\n",
      "loss:  0.8318190574645996\n",
      "loss:  0.7664441466331482\n",
      "loss:  0.9287364482879639\n",
      "loss:  0.9634000658988953\n",
      "loss:  0.8591669201850891\n",
      "loss:  0.8347764015197754\n",
      "loss:  0.7463330626487732\n",
      "loss:  0.8400276899337769\n",
      "loss:  0.7721843123435974\n",
      "loss:  0.8655110597610474\n",
      "loss:  0.8426136374473572\n",
      "loss:  0.794902503490448\n",
      "loss:  0.8635473251342773\n",
      "loss:  0.741260826587677\n",
      "loss:  0.8489654064178467\n",
      "loss:  0.7565582990646362\n",
      "loss:  0.8461757898330688\n",
      "loss:  0.733907163143158\n",
      "loss:  0.6736621260643005\n",
      "loss:  0.8775206208229065\n",
      "loss:  0.8110911846160889\n",
      "loss:  0.7595382928848267\n",
      "loss:  0.7042601108551025\n",
      "loss:  0.7629601955413818\n",
      "loss:  0.6816546320915222\n",
      "loss:  0.9414728283882141\n",
      "loss:  0.771079957485199\n",
      "loss:  0.7582611441612244\n",
      "loss:  0.6563432216644287\n",
      "loss:  0.7454485893249512\n",
      "loss:  0.7577893137931824\n",
      "loss:  0.753481388092041\n",
      "loss:  0.6940429210662842\n",
      "loss:  0.7266477942466736\n",
      "loss:  0.751071572303772\n",
      "loss:  0.7931378483772278\n",
      "loss:  0.7805377840995789\n",
      "loss:  0.7648165225982666\n",
      "loss:  0.7475964426994324\n",
      "loss:  0.8120013475418091\n",
      "loss:  0.6617591977119446\n",
      "loss:  0.928358793258667\n",
      "loss:  0.7710176110267639\n",
      "loss:  0.7412883639335632\n",
      "loss:  0.633629560470581\n",
      "loss:  0.7443853616714478\n",
      "loss:  0.7282703518867493\n",
      "loss:  0.7475503087043762\n",
      "loss:  0.7659821510314941\n",
      "loss:  0.856242299079895\n",
      "loss:  0.7548875212669373\n",
      "loss:  0.8019930124282837\n",
      "loss:  0.7500129342079163\n",
      "loss:  0.7031926512718201\n",
      "loss:  0.7595652341842651\n",
      "loss:  0.7659397721290588\n",
      "loss:  0.5860028862953186\n",
      "loss:  0.9939641356468201\n",
      "loss:  0.6881823539733887\n",
      "loss:  0.7106664180755615\n",
      "loss:  0.8784689903259277\n",
      "loss:  0.8199668526649475\n",
      "loss:  0.7659627199172974\n",
      "loss:  0.8739137649536133\n",
      "loss:  0.7149010896682739\n",
      "loss:  0.7693200707435608\n",
      "loss:  0.8672499060630798\n",
      "loss:  0.8521338105201721\n",
      "loss:  0.8782756924629211\n",
      "loss:  0.7622622847557068\n",
      "loss:  0.7778990864753723\n",
      "loss:  0.7954069375991821\n",
      "loss:  0.6840444207191467\n",
      "loss:  0.8401159644126892\n",
      "loss:  0.7316780090332031\n",
      "loss:  0.8343756794929504\n",
      "loss:  0.7422520518302917\n",
      "loss:  0.880921483039856\n",
      "loss:  0.8139199614524841\n",
      "loss:  0.9352855086326599\n",
      "loss:  0.9099503755569458\n",
      "loss:  0.8536607623100281\n",
      "loss:  0.7113189697265625\n",
      "loss:  0.7803288102149963\n",
      "loss:  0.8192307949066162\n",
      "loss:  0.7387106418609619\n",
      "loss:  0.7857491374015808\n",
      "loss:  0.6988467574119568\n",
      "loss:  0.7744672894477844\n",
      "loss:  0.7271859049797058\n",
      "loss:  0.8398397564888\n",
      "loss:  0.6230280995368958\n",
      "loss:  0.7128896713256836\n",
      "loss:  0.8103827834129333\n",
      "loss:  0.6233428120613098\n",
      "loss:  0.7319966554641724\n",
      "loss:  0.7849240303039551\n",
      "loss:  0.7298314571380615\n",
      "loss:  0.7066309452056885\n",
      "loss:  0.8692324161529541\n",
      "loss:  0.6587851047515869\n",
      "loss:  0.7086272835731506\n",
      "loss:  0.7853169441223145\n",
      "loss:  0.6788410544395447\n",
      "loss:  0.8277186751365662\n",
      "loss:  0.7341654896736145\n",
      "loss:  0.8820053935050964\n",
      "loss:  0.9378827810287476\n",
      "loss:  0.7361494898796082\n",
      "loss:  0.674241840839386\n",
      "loss:  0.6718365550041199\n",
      "loss:  0.7062553763389587\n",
      "loss:  0.8884046673774719\n",
      "loss:  0.7271226048469543\n",
      "loss:  0.8077600598335266\n",
      "loss:  0.7012365460395813\n",
      "loss:  0.7906981706619263\n",
      "loss:  0.6744524836540222\n",
      "loss:  0.6748904585838318\n",
      "loss:  0.8646593689918518\n",
      "loss:  0.8413498401641846\n",
      "loss:  0.9072868824005127\n",
      "loss:  0.8194899559020996\n",
      "loss:  0.5996968150138855\n",
      "loss:  0.8679760098457336\n",
      "loss:  0.7803043723106384\n",
      "loss:  0.6265652775764465\n",
      "loss:  0.8131256103515625\n",
      "loss:  0.9963265657424927\n",
      "loss:  0.735009491443634\n",
      "loss:  0.7045074105262756\n",
      "loss:  0.646134614944458\n",
      "loss:  0.7607830762863159\n",
      "loss:  0.6609752774238586\n",
      "loss:  0.7439676523208618\n",
      "loss:  0.9521883130073547\n",
      "loss:  0.90152508020401\n",
      "loss:  0.9805818200111389\n",
      "loss:  0.8499776721000671\n",
      "loss:  0.6972430944442749\n",
      "loss:  0.6917039752006531\n",
      "loss:  0.6938853859901428\n",
      "loss:  0.7481399774551392\n",
      "loss:  0.6747207045555115\n",
      "loss:  0.6282933354377747\n",
      "loss:  0.7700509428977966\n",
      "loss:  0.8614507913589478\n",
      "loss:  0.7878748774528503\n",
      "loss:  0.8508589267730713\n",
      "loss:  0.6143938302993774\n",
      "loss:  0.6598414778709412\n",
      "loss:  0.7858291864395142\n",
      "loss:  0.7235251665115356\n",
      "loss:  0.7822971940040588\n",
      "loss:  0.6494587063789368\n",
      "loss:  0.8019516468048096\n",
      "loss:  0.778718113899231\n",
      "loss:  0.746625542640686\n",
      "loss:  0.6644940972328186\n",
      "loss:  0.6856694221496582\n",
      "loss:  0.7976763248443604\n",
      "loss:  0.6869621872901917\n",
      "loss:  0.7629308104515076\n",
      "loss:  0.7102643251419067\n",
      "loss:  0.6719866394996643\n",
      "loss:  0.791368842124939\n",
      "loss:  0.7731331586837769\n",
      "loss:  0.772731363773346\n",
      "loss:  0.6520733833312988\n",
      "loss:  0.5507854223251343\n",
      "loss:  0.8011161088943481\n",
      "loss:  0.7299531698226929\n",
      "loss:  0.7352499961853027\n",
      "loss:  0.6865058541297913\n",
      "loss:  0.7370259165763855\n",
      "loss:  0.6445737481117249\n",
      "loss:  0.7325106263160706\n",
      "loss:  0.7724819183349609\n",
      "loss:  0.6633810997009277\n",
      "loss:  0.7063655853271484\n",
      "loss:  0.8955029845237732\n",
      "loss:  0.6637950539588928\n",
      "loss:  0.7318078279495239\n",
      "loss:  0.9093453884124756\n",
      "loss:  0.772996723651886\n",
      "loss:  0.7156849503517151\n",
      "loss:  0.5953919291496277\n",
      "loss:  0.7267647981643677\n",
      "loss:  0.6906689405441284\n",
      "loss:  0.613528847694397\n",
      "loss:  0.7522013187408447\n",
      "loss:  0.8723551034927368\n",
      "loss:  0.7271979451179504\n",
      "loss:  0.8515474796295166\n",
      "loss:  0.777115523815155\n",
      "loss:  0.6613430380821228\n",
      "loss:  0.6616179943084717\n",
      "loss:  0.7678289413452148\n",
      "loss:  0.8217023015022278\n",
      "loss:  0.8497382402420044\n",
      "loss:  0.7733796238899231\n",
      "loss:  0.6073299646377563\n",
      "loss:  0.9034755825996399\n",
      "loss:  0.759338915348053\n",
      "loss:  0.7272778153419495\n",
      "loss:  0.7293721437454224\n",
      "loss:  0.6607345342636108\n",
      "loss:  0.7351359128952026\n",
      "loss:  0.8283582925796509\n",
      "loss:  0.6801655292510986\n",
      "loss:  0.7078198790550232\n",
      "loss:  0.8221470713615417\n",
      "loss:  0.6414299011230469\n",
      "loss:  0.836862325668335\n",
      "loss:  0.7341630458831787\n",
      "loss:  0.7268476486206055\n",
      "loss:  0.7755374312400818\n",
      "loss:  0.6910565495491028\n",
      "loss:  0.7775874137878418\n",
      "loss:  0.6331814527511597\n",
      "loss:  0.8188337683677673\n",
      "loss:  0.7319191694259644\n",
      "loss:  0.7650822401046753\n",
      "loss:  0.7343383431434631\n",
      "loss:  0.8757876753807068\n",
      "loss:  0.650009036064148\n",
      "loss:  0.7782297134399414\n",
      "loss:  0.791638970375061\n",
      "loss:  0.8261299133300781\n",
      "loss:  0.7878236770629883\n",
      "loss:  0.6434637904167175\n",
      "loss:  0.7342655062675476\n",
      "loss:  0.7339685559272766\n",
      "loss:  0.754275918006897\n",
      "loss:  0.6863622665405273\n",
      "loss:  0.7251405119895935\n",
      "loss:  0.6993682384490967\n",
      "loss:  0.7717437744140625\n",
      "loss:  0.6405414342880249\n",
      "loss:  0.6336086392402649\n",
      "loss:  0.7734885215759277\n",
      "loss:  0.8095025420188904\n",
      "loss:  0.7336124181747437\n",
      "loss:  0.7812404632568359\n",
      "loss:  0.8281115293502808\n",
      "loss:  0.7614250183105469\n",
      "loss:  0.7308110594749451\n",
      "loss:  0.7545505166053772\n",
      "loss:  0.8299041986465454\n",
      "loss:  0.7811935544013977\n",
      "loss:  0.6293432116508484\n",
      "loss:  0.7535900473594666\n",
      "loss:  0.6603649854660034\n",
      "loss:  0.6575916409492493\n",
      "loss:  0.7266435623168945\n",
      "loss:  0.706618070602417\n",
      "loss:  0.6600624322891235\n",
      "loss:  0.6736436486244202\n",
      "loss:  0.7907448410987854\n",
      "loss:  0.6036779880523682\n",
      "loss:  0.6946128606796265\n",
      "loss:  0.7089492082595825\n",
      "loss:  0.6556147933006287\n",
      "loss:  0.5145061016082764\n",
      "loss:  0.7697890996932983\n",
      "loss:  0.7902836203575134\n",
      "loss:  0.7017824053764343\n",
      "loss:  0.8122485876083374\n",
      "loss:  0.6549580693244934\n",
      "loss:  0.8237534165382385\n",
      "loss:  0.7945923209190369\n",
      "loss:  0.7761901617050171\n",
      "loss:  0.6825137734413147\n",
      "loss:  0.7716197371482849\n",
      "loss:  0.7395913600921631\n",
      "loss:  0.8599073886871338\n",
      "loss:  0.7878507375717163\n",
      "loss:  0.7526560425758362\n",
      "loss:  0.7770050764083862\n",
      "loss:  0.7695119380950928\n",
      "loss:  0.6597378849983215\n",
      "loss:  0.5635755658149719\n",
      "loss:  0.6474660634994507\n",
      "loss:  0.7623529434204102\n",
      "loss:  0.7382742166519165\n",
      "loss:  0.5994101762771606\n",
      "loss:  0.7051012516021729\n",
      "loss:  0.729010283946991\n",
      "loss:  0.764466404914856\n",
      "loss:  0.5687076449394226\n",
      "loss:  0.7424566745758057\n",
      "loss:  0.6969279646873474\n",
      "loss:  0.7451744079589844\n",
      "loss:  0.6016715168952942\n",
      "loss:  0.5954290628433228\n",
      "loss:  0.8011190295219421\n",
      "loss:  0.7622326612472534\n",
      "loss:  0.675541341304779\n",
      "loss:  0.6520118713378906\n",
      "loss:  0.7462651133537292\n",
      "loss:  0.7038775682449341\n",
      "loss:  0.6608158946037292\n",
      "loss:  0.6179324984550476\n",
      "loss:  0.7290298938751221\n",
      "loss:  0.6804473996162415\n",
      "loss:  0.7478659749031067\n",
      "loss:  0.7520976066589355\n",
      "loss:  0.7171488404273987\n",
      "loss:  0.6416724324226379\n",
      "loss:  0.7000463604927063\n",
      "loss:  0.7370883226394653\n",
      "loss:  0.6545913219451904\n",
      "loss:  0.8192551732063293\n",
      "loss:  0.6365420818328857\n",
      "loss:  0.5831223130226135\n",
      "loss:  0.6994823217391968\n",
      "loss:  0.6242280602455139\n",
      "loss:  0.7553666234016418\n",
      "loss:  0.7524425387382507\n",
      "loss:  0.7839064598083496\n",
      "loss:  0.7914058566093445\n",
      "loss:  0.6700078845024109\n",
      "loss:  0.5963242650032043\n",
      "loss:  0.6564393043518066\n",
      "loss:  0.7450782060623169\n",
      "loss:  0.6930111646652222\n",
      "loss:  0.6161585450172424\n",
      "loss:  0.7918531894683838\n",
      "loss:  0.6485744118690491\n",
      "loss:  0.8056224584579468\n",
      "loss:  0.8568273782730103\n",
      "loss:  0.7457373738288879\n",
      "loss:  0.7708970308303833\n",
      "loss:  0.6392647624015808\n",
      "loss:  0.8396270871162415\n",
      "loss:  0.7162208557128906\n",
      "loss:  0.6933009028434753\n",
      "loss:  0.6882045269012451\n",
      "loss:  0.7399478554725647\n",
      "loss:  0.7397580146789551\n",
      "loss:  0.5326355695724487\n",
      "loss:  0.6790058612823486\n",
      "loss:  0.7390377521514893\n",
      "loss:  0.8016849160194397\n",
      "loss:  0.7304463386535645\n",
      "loss:  0.6885760426521301\n",
      "loss:  0.7620917558670044\n",
      "loss:  0.7897637486457825\n",
      "loss:  0.6461572647094727\n",
      "loss:  0.8079725503921509\n",
      "loss:  0.6362040042877197\n",
      "loss:  0.6260673403739929\n",
      "loss:  0.6212942600250244\n",
      "loss:  0.7430465817451477\n",
      "loss:  0.7778205871582031\n",
      "loss:  0.7262662649154663\n",
      "loss:  0.7130337953567505\n",
      "loss:  0.6437013745307922\n",
      "loss:  0.6707855463027954\n",
      "loss:  0.681186854839325\n",
      "loss:  0.6724338531494141\n",
      "loss:  0.7544388771057129\n",
      "loss:  0.6641385555267334\n",
      "loss:  0.7052023410797119\n",
      "loss:  0.6612716317176819\n",
      "loss:  0.6350315809249878\n",
      "loss:  0.8781942129135132\n",
      "loss:  0.6496762037277222\n",
      "loss:  0.7578986883163452\n",
      "loss:  0.8485186696052551\n",
      "loss:  0.6870290040969849\n",
      "loss:  0.7134836316108704\n",
      "loss:  0.5742244124412537\n",
      "loss:  0.6693220734596252\n",
      "loss:  0.5440899729728699\n",
      "loss:  0.5963901281356812\n",
      "loss:  0.7264296412467957\n",
      "loss:  0.7967499494552612\n",
      "loss:  0.701412558555603\n",
      "loss:  0.8456814289093018\n",
      "loss:  0.6698344349861145\n",
      "loss:  0.5214483737945557\n",
      "loss:  0.7585490942001343\n",
      "loss:  0.7072423696517944\n",
      "loss:  0.6679882407188416\n",
      "loss:  0.7704596519470215\n",
      "loss:  0.6736891269683838\n",
      "loss:  0.5127434134483337\n",
      "loss:  0.6706201434135437\n",
      "loss:  0.6037673354148865\n",
      "loss:  0.5988073348999023\n",
      "loss:  0.7572143077850342\n",
      "loss:  0.6494370102882385\n",
      "loss:  0.7592299580574036\n",
      "loss:  0.5297791361808777\n",
      "loss:  0.5827441811561584\n",
      "loss:  0.7143293619155884\n",
      "loss:  0.6343737840652466\n",
      "loss:  0.7422564029693604\n",
      "loss:  0.5327551364898682\n",
      "loss:  0.6965392827987671\n",
      "loss:  0.743762195110321\n",
      "loss:  0.6139713525772095\n",
      "loss:  0.6585571765899658\n",
      "loss:  0.7669326663017273\n",
      "loss:  0.6107881665229797\n",
      "loss:  0.654363214969635\n",
      "loss:  0.5917988419532776\n",
      "loss:  0.6599010825157166\n",
      "loss:  0.7604813575744629\n",
      "loss:  0.7585890889167786\n",
      "loss:  0.7188315391540527\n",
      "loss:  0.700658917427063\n",
      "loss:  0.6462868452072144\n",
      "loss:  0.8317497372627258\n",
      "loss:  0.6482121348381042\n",
      "loss:  0.6236841678619385\n",
      "loss:  0.8533957600593567\n",
      "loss:  0.6995188593864441\n",
      "loss:  0.5965291857719421\n",
      "loss:  0.7247668504714966\n",
      "loss:  0.5748884677886963\n",
      "loss:  0.7381965517997742\n",
      "loss:  0.5893265604972839\n",
      "loss:  0.6018313765525818\n",
      "loss:  0.6948936581611633\n",
      "loss:  0.6365007162094116\n",
      "loss:  0.9115065336227417\n",
      "loss:  0.7470126152038574\n",
      "loss:  0.6006669402122498\n",
      "loss:  0.7578915953636169\n",
      "loss:  0.6447527408599854\n",
      "loss:  0.571546733379364\n",
      "loss:  0.679421603679657\n",
      "loss:  0.8030357360839844\n",
      "loss:  0.6827564239501953\n",
      "loss:  0.7629823684692383\n",
      "loss:  0.637954592704773\n",
      "loss:  0.6966192126274109\n",
      "loss:  0.6788963079452515\n",
      "loss:  0.7729434967041016\n",
      "loss:  0.5728338360786438\n",
      "loss:  0.666938841342926\n",
      "loss:  0.7150668501853943\n",
      "loss:  0.6992994546890259\n",
      "loss:  0.8321056962013245\n",
      "loss:  0.6974057555198669\n",
      "loss:  0.9548954963684082\n",
      "loss:  0.7778747081756592\n",
      "loss:  0.641883134841919\n",
      "loss:  0.577944278717041\n",
      "loss:  0.6051350235939026\n",
      "loss:  0.6482660174369812\n",
      "loss:  0.6662200689315796\n",
      "loss:  0.7276422381401062\n",
      "loss:  0.638738751411438\n",
      "loss:  0.668812096118927\n",
      "loss:  0.6375988125801086\n",
      "loss:  0.6530442237854004\n",
      "loss:  0.603642463684082\n",
      "loss:  0.6747444272041321\n",
      "loss:  0.6489770412445068\n",
      "loss:  0.7298164367675781\n",
      "loss:  0.5603903532028198\n",
      "loss:  0.7453364729881287\n",
      "loss:  0.5972743630409241\n",
      "loss:  0.7116535902023315\n",
      "loss:  0.6243986487388611\n",
      "loss:  0.5690141916275024\n",
      "loss:  0.5462682247161865\n",
      "loss:  0.64944988489151\n",
      "loss:  0.6145655512809753\n",
      "loss:  0.7375765442848206\n",
      "loss:  0.7580832242965698\n",
      "loss:  0.6866890788078308\n",
      "loss:  0.7489592432975769\n",
      "loss:  0.577565610408783\n",
      "loss:  0.7254313230514526\n",
      "loss:  0.6621737480163574\n",
      "loss:  0.8141838312149048\n",
      "loss:  0.6879559755325317\n",
      "loss:  0.7319172620773315\n",
      "loss:  0.7158012390136719\n",
      "loss:  0.766764760017395\n",
      "loss:  0.6148857474327087\n",
      "loss:  0.6840218305587769\n",
      "loss:  0.6328812837600708\n",
      "loss:  0.623132050037384\n",
      "loss:  0.5323302745819092\n",
      "loss:  0.7233330607414246\n",
      "loss:  0.7002744674682617\n",
      "loss:  0.7373049855232239\n",
      "loss:  0.6183303594589233\n",
      "loss:  0.5401635766029358\n",
      "loss:  0.7476329207420349\n",
      "loss:  0.7652155756950378\n",
      "loss:  0.5684189200401306\n",
      "loss:  0.7153561115264893\n",
      "loss:  0.6471145749092102\n",
      "loss:  0.6780365705490112\n",
      "loss:  0.6022582650184631\n",
      "loss:  0.6505448222160339\n",
      "loss:  0.6925954818725586\n",
      "loss:  0.5775702595710754\n",
      "loss:  0.631904661655426\n",
      "loss:  0.6814782023429871\n",
      "loss:  0.814773440361023\n",
      "loss:  0.5758021473884583\n",
      "loss:  0.7303268313407898\n",
      "loss:  0.628601610660553\n",
      "loss:  0.6743377447128296\n",
      "loss:  0.7307572364807129\n",
      "loss:  0.6441510915756226\n",
      "loss:  0.7287304997444153\n",
      "loss:  0.6294699311256409\n",
      "loss:  0.7021154761314392\n",
      "loss:  0.7809538841247559\n",
      "loss:  0.6578018069267273\n",
      "loss:  0.814500629901886\n",
      "loss:  0.6964003443717957\n",
      "loss:  0.6378675699234009\n",
      "loss:  0.5541673302650452\n",
      "loss:  0.8330656290054321\n",
      "loss:  0.7374615669250488\n",
      "loss:  0.6288586258888245\n",
      "loss:  0.6792349815368652\n",
      "loss:  0.6164517402648926\n",
      "loss:  0.7302498817443848\n",
      "loss:  0.7304317951202393\n",
      "loss:  0.6778441667556763\n",
      "loss:  0.6497498750686646\n",
      "loss:  0.7412872314453125\n",
      "loss:  0.62260502576828\n",
      "loss:  0.7345248460769653\n",
      "loss:  0.45112553238868713\n",
      "loss:  0.6844243407249451\n",
      "loss:  0.6315450668334961\n",
      "loss:  0.5913137793540955\n",
      "loss:  0.6184245944023132\n",
      "loss:  0.5789886713027954\n",
      "loss:  0.7430731058120728\n",
      "loss:  0.5235755443572998\n",
      "loss:  0.7181836366653442\n",
      "loss:  0.7066376805305481\n",
      "loss:  0.6769967675209045\n",
      "loss:  0.7756202220916748\n",
      "loss:  0.6204557418823242\n",
      "loss:  0.6841893196105957\n",
      "loss:  0.5572298765182495\n",
      "loss:  0.7768356204032898\n",
      "loss:  0.5347679853439331\n",
      "loss:  0.6018838286399841\n",
      "loss:  0.6506087183952332\n",
      "loss:  0.6707410216331482\n",
      "loss:  0.6096187829971313\n",
      "loss:  0.7262900471687317\n",
      "loss:  0.5621137022972107\n",
      "loss:  0.6603651642799377\n",
      "loss:  0.6455532312393188\n",
      "loss:  0.705096960067749\n",
      "loss:  0.658241868019104\n",
      "loss:  0.6929391026496887\n",
      "loss:  0.6497247219085693\n",
      "loss:  0.6632124185562134\n",
      "loss:  0.7395172715187073\n",
      "loss:  0.705023467540741\n",
      "loss:  0.749785304069519\n",
      "loss:  0.5745231509208679\n",
      "loss:  0.5560902953147888\n",
      "loss:  0.668048083782196\n",
      "loss:  0.7152246832847595\n",
      "loss:  0.8288765549659729\n",
      "loss:  0.7976168394088745\n",
      "loss:  0.6737837791442871\n",
      "loss:  0.7958974838256836\n",
      "loss:  0.7269340753555298\n",
      "loss:  0.58780837059021\n",
      "loss:  0.655975341796875\n",
      "loss:  0.7185229063034058\n",
      "loss:  0.641349732875824\n",
      "loss:  0.6779161095619202\n",
      "loss:  0.6767317652702332\n",
      "loss:  0.724172830581665\n",
      "loss:  0.7439213991165161\n",
      "loss:  0.6670545935630798\n",
      "loss:  0.7010232210159302\n",
      "loss:  0.690553605556488\n",
      "loss:  0.6907157897949219\n",
      "loss:  0.5051344633102417\n",
      "loss:  0.6381974816322327\n",
      "loss:  0.5409638285636902\n",
      "loss:  0.7027285099029541\n",
      "loss:  0.5662000775337219\n",
      "loss:  0.5744146704673767\n",
      "loss:  0.5901753306388855\n",
      "loss:  0.6652449369430542\n",
      "loss:  0.6209235191345215\n",
      "loss:  0.8027443289756775\n",
      "loss:  0.780636727809906\n",
      "loss:  0.622173547744751\n",
      "loss:  0.6708042621612549\n",
      "loss:  0.5836102366447449\n",
      "loss:  0.5233474373817444\n",
      "loss:  0.6563221216201782\n",
      "loss:  0.75450199842453\n",
      "loss:  0.7670652270317078\n",
      "loss:  0.5451489686965942\n",
      "loss:  0.6612144708633423\n",
      "loss:  0.5731229782104492\n",
      "loss:  0.8066074848175049\n",
      "loss:  0.6164465546607971\n",
      "loss:  0.5655019283294678\n",
      "loss:  0.5366425514221191\n",
      "loss:  0.5919040441513062\n",
      "loss:  0.6985034942626953\n",
      "loss:  0.7087900638580322\n",
      "loss:  0.7110604643821716\n",
      "loss:  0.6654723882675171\n",
      "loss:  0.6247479915618896\n",
      "loss:  0.569162130355835\n",
      "loss:  0.7947920560836792\n",
      "loss:  0.7159858345985413\n",
      "loss:  0.5352686047554016\n",
      "loss:  0.5764721035957336\n",
      "loss:  0.6796687841415405\n",
      "loss:  0.703268826007843\n",
      "loss:  0.8066858053207397\n",
      "loss:  0.5644657611846924\n",
      "loss:  0.5698691010475159\n",
      "loss:  0.7672592997550964\n",
      "loss:  0.68619304895401\n",
      "loss:  0.7021977305412292\n",
      "loss:  0.688568115234375\n",
      "loss:  0.6965322494506836\n",
      "loss:  0.8004204034805298\n",
      "loss:  0.546099066734314\n",
      "loss:  0.7677128911018372\n",
      "loss:  0.8026978373527527\n",
      "loss:  0.5559898614883423\n",
      "loss:  0.663159191608429\n",
      "loss:  0.7032349109649658\n",
      "loss:  0.5916779041290283\n",
      "loss:  0.7669637203216553\n",
      "loss:  0.6485280394554138\n",
      "loss:  0.592786967754364\n",
      "loss:  0.520467221736908\n",
      "loss:  0.6941168308258057\n",
      "loss:  0.5839479565620422\n",
      "loss:  0.552210807800293\n",
      "loss:  0.7591200470924377\n",
      "loss:  0.6335205435752869\n",
      "loss:  0.5143163204193115\n",
      "loss:  0.7152017951011658\n",
      "loss:  0.6083303689956665\n",
      "loss:  0.6579004526138306\n",
      "loss:  0.6785144805908203\n",
      "loss:  0.507408618927002\n",
      "loss:  0.6241222023963928\n",
      "loss:  0.6885925531387329\n",
      "loss:  0.7390825152397156\n",
      "loss:  0.7704050540924072\n",
      "loss:  0.6338610649108887\n",
      "loss:  0.5487046241760254\n",
      "loss:  0.8023024201393127\n",
      "loss:  0.7465937733650208\n",
      "loss:  0.7954061031341553\n",
      "loss:  0.6477553844451904\n",
      "loss:  0.5118343830108643\n",
      "loss:  0.704276442527771\n",
      "loss:  0.6295183897018433\n",
      "loss:  0.4898207187652588\n",
      "loss:  0.6766111254692078\n",
      "loss:  0.7120732665061951\n",
      "loss:  0.6022657155990601\n",
      "loss:  0.6879671812057495\n",
      "loss:  0.4784153997898102\n",
      "loss:  0.7889167666435242\n",
      "loss:  0.7165027856826782\n",
      "loss:  0.5269865989685059\n",
      "loss:  0.5475429892539978\n",
      "loss:  0.6853030920028687\n",
      "loss:  0.5069212317466736\n",
      "loss:  0.7293635010719299\n",
      "loss:  0.6388491988182068\n",
      "loss:  0.596606969833374\n",
      "loss:  0.6553438305854797\n",
      "loss:  0.7150155305862427\n",
      "loss:  0.5111963748931885\n",
      "loss:  0.821558952331543\n",
      "loss:  0.7157130241394043\n",
      "loss:  0.7352958917617798\n",
      "loss:  0.557077944278717\n",
      "loss:  0.6826098561286926\n",
      "loss:  0.5154759287834167\n",
      "loss:  0.6638122200965881\n",
      "loss:  0.6956014037132263\n",
      "loss:  0.6541531682014465\n",
      "loss:  0.6803020238876343\n",
      "loss:  0.5744321942329407\n",
      "loss:  0.5903307795524597\n",
      "loss:  0.5793181657791138\n",
      "loss:  0.70235675573349\n",
      "loss:  0.6244084239006042\n",
      "loss:  0.5955809950828552\n",
      "loss:  0.6292015910148621\n",
      "loss:  0.7179365158081055\n",
      "loss:  0.5982977747917175\n",
      "loss:  0.6827762722969055\n",
      "loss:  0.6139947175979614\n",
      "loss:  0.6329247355461121\n",
      "loss:  0.8957769274711609\n",
      "loss:  0.7145402431488037\n",
      "loss:  0.5109872221946716\n",
      "loss:  0.6157491207122803\n",
      "loss:  0.6047747135162354\n",
      "loss:  0.648402214050293\n",
      "loss:  0.6869924664497375\n",
      "loss:  0.5461075305938721\n",
      "loss:  0.5906171202659607\n",
      "loss:  0.6287119388580322\n",
      "loss:  0.6012043952941895\n",
      "loss:  0.511724591255188\n",
      "loss:  0.7322481870651245\n",
      "loss:  0.6014924049377441\n",
      "loss:  0.5428521037101746\n",
      "loss:  0.6970900893211365\n",
      "loss:  0.5040879249572754\n",
      "loss:  0.6488085985183716\n",
      "loss:  0.6991710066795349\n",
      "loss:  0.6483728289604187\n",
      "loss:  0.5832410454750061\n",
      "loss:  0.6613313555717468\n",
      "loss:  0.7454854846000671\n",
      "loss:  0.6426676511764526\n",
      "loss:  0.6920390725135803\n",
      "loss:  0.5849886536598206\n",
      "loss:  0.6215618252754211\n",
      "loss:  0.614253580570221\n",
      "loss:  0.5890297293663025\n",
      "loss:  0.5924020409584045\n",
      "loss:  0.74232017993927\n",
      "loss:  0.4793156683444977\n",
      "loss:  0.6679586172103882\n",
      "loss:  0.6099767684936523\n",
      "loss:  0.5899354815483093\n",
      "loss:  0.7538268566131592\n",
      "loss:  0.5706266760826111\n",
      "loss:  0.7487109899520874\n",
      "loss:  0.5276007652282715\n",
      "loss:  0.6874029040336609\n",
      "loss:  0.6521447896957397\n",
      "loss:  0.7155935168266296\n",
      "loss:  0.7356047630310059\n",
      "loss:  0.6041017770767212\n",
      "loss:  0.7349115014076233\n",
      "loss:  0.6339545249938965\n",
      "loss:  0.5777977108955383\n",
      "loss:  0.61444091796875\n",
      "loss:  0.6830915808677673\n",
      "loss:  0.6027087569236755\n",
      "loss:  0.6781862378120422\n",
      "loss:  0.5888301730155945\n",
      "loss:  0.5553394556045532\n",
      "loss:  0.6988238096237183\n",
      "loss:  0.7843595743179321\n",
      "loss:  0.662359356880188\n",
      "loss:  0.5140585899353027\n",
      "loss:  0.7433872818946838\n",
      "loss:  0.6995990872383118\n",
      "loss:  0.6290417313575745\n",
      "loss:  0.728725016117096\n",
      "loss:  0.5002680420875549\n",
      "loss:  0.5857452154159546\n",
      "loss:  0.6458570957183838\n",
      "loss:  0.6255995035171509\n",
      "loss:  0.5831248760223389\n",
      "loss:  0.5752812027931213\n",
      "loss:  0.5481317043304443\n",
      "loss:  0.7632552981376648\n",
      "loss:  0.6482840776443481\n",
      "loss:  0.5119754076004028\n",
      "loss:  0.6699484586715698\n",
      "loss:  0.7396719455718994\n",
      "loss:  0.5421682596206665\n",
      "loss:  0.6335539817810059\n",
      "loss:  0.5650410652160645\n",
      "loss:  0.6189437508583069\n",
      "loss:  0.7016909718513489\n",
      "loss:  0.6281989216804504\n",
      "loss:  0.6448389887809753\n",
      "loss:  0.7128340005874634\n",
      "loss:  0.5247924327850342\n",
      "loss:  0.5064267516136169\n",
      "loss:  0.720063328742981\n",
      "loss:  0.6384492516517639\n",
      "loss:  0.80805504322052\n",
      "loss:  0.7825626730918884\n",
      "loss:  0.7004199624061584\n",
      "loss:  0.5745235681533813\n",
      "loss:  0.7136163711547852\n",
      "loss:  0.6428545713424683\n",
      "loss:  0.5714105367660522\n",
      "loss:  0.6039953231811523\n",
      "loss:  0.7030683755874634\n",
      "loss:  0.6169106364250183\n",
      "loss:  0.6612421870231628\n",
      "loss:  0.7369471788406372\n",
      "loss:  0.7510443925857544\n",
      "loss:  0.6893788576126099\n",
      "loss:  0.6104761958122253\n",
      "loss:  0.7515218257904053\n",
      "loss:  0.8478901386260986\n",
      "loss:  0.667037308216095\n",
      "loss:  0.6253833770751953\n",
      "loss:  0.577822208404541\n",
      "loss:  0.6656475067138672\n",
      "loss:  0.4661461114883423\n",
      "loss:  0.8458710312843323\n",
      "loss:  0.5956724286079407\n",
      "loss:  0.4848759174346924\n",
      "loss:  0.6571961045265198\n",
      "loss:  0.7276590466499329\n",
      "loss:  0.7200065851211548\n",
      "loss:  0.7123191356658936\n",
      "loss:  0.6207190155982971\n",
      "loss:  0.6408713459968567\n",
      "loss:  0.6177176237106323\n",
      "loss:  0.5494022965431213\n",
      "loss:  0.5816672444343567\n",
      "loss:  0.7039578557014465\n",
      "loss:  0.6956272721290588\n",
      "loss:  0.5169145464897156\n",
      "loss:  0.6205422878265381\n",
      "loss:  0.5819075703620911\n",
      "loss:  0.546533465385437\n",
      "loss:  0.6359351277351379\n",
      "loss:  0.8085729479789734\n",
      "loss:  0.5182236433029175\n",
      "loss:  0.6095850467681885\n",
      "loss:  0.5808825492858887\n",
      "loss:  0.6186333894729614\n",
      "loss:  0.5918228626251221\n",
      "loss:  0.6972367763519287\n",
      "loss:  0.5332983136177063\n",
      "loss:  0.5927942395210266\n",
      "loss:  0.568228542804718\n",
      "loss:  0.5180338621139526\n",
      "loss:  0.5888193845748901\n",
      "loss:  0.5601710081100464\n",
      "loss:  0.6176186203956604\n",
      "loss:  0.6453658938407898\n",
      "loss:  0.6758597493171692\n",
      "loss:  0.5905251502990723\n",
      "loss:  0.6354498267173767\n",
      "loss:  0.5317882895469666\n",
      "loss:  0.49896910786628723\n",
      "loss:  0.6226860880851746\n",
      "loss:  0.5551677942276001\n",
      "loss:  0.6946123242378235\n",
      "loss:  0.5543995499610901\n",
      "loss:  0.49974292516708374\n",
      "loss:  0.5651161074638367\n",
      "loss:  0.5349051356315613\n",
      "loss:  0.858671247959137\n",
      "loss:  0.46642211079597473\n",
      "loss:  0.5324026942253113\n",
      "loss:  0.5349944233894348\n",
      "loss:  0.48348408937454224\n",
      "loss:  0.5334644913673401\n",
      "loss:  0.6315078139305115\n",
      "loss:  0.7200070023536682\n",
      "loss:  0.6622036099433899\n",
      "loss:  0.5277644395828247\n",
      "loss:  0.5689371228218079\n",
      "loss:  0.5319477915763855\n",
      "loss:  0.5652405619621277\n",
      "loss:  0.6221550703048706\n",
      "loss:  0.4841911196708679\n",
      "loss:  0.6250980496406555\n",
      "loss:  0.5311974287033081\n",
      "loss:  0.8113075494766235\n",
      "loss:  0.5021446943283081\n",
      "loss:  0.6384217143058777\n",
      "loss:  0.48938602209091187\n",
      "loss:  0.5209632515907288\n",
      "loss:  0.5521599650382996\n",
      "loss:  0.5607678294181824\n",
      "loss:  0.629816472530365\n",
      "loss:  0.6155504584312439\n",
      "loss:  0.6036063432693481\n",
      "loss:  0.5284045934677124\n",
      "loss:  0.6068975925445557\n",
      "loss:  0.5119106769561768\n",
      "loss:  0.7423983812332153\n",
      "loss:  0.5732064247131348\n",
      "loss:  0.6857889294624329\n",
      "loss:  0.6137891411781311\n",
      "loss:  0.7147482633590698\n",
      "loss:  0.621950626373291\n",
      "loss:  0.6473236680030823\n",
      "loss:  0.5769176483154297\n",
      "loss:  0.5942338705062866\n",
      "loss:  0.8219082951545715\n",
      "loss:  0.6315053105354309\n",
      "loss:  0.6297245025634766\n",
      "loss:  0.6721644997596741\n",
      "loss:  0.5277905464172363\n",
      "loss:  0.6139218211174011\n",
      "loss:  0.5180704593658447\n",
      "loss:  0.6374813318252563\n",
      "loss:  0.541941225528717\n",
      "loss:  0.6534080505371094\n",
      "loss:  0.6268509030342102\n",
      "loss:  0.7221061587333679\n",
      "loss:  0.5199216604232788\n",
      "loss:  0.5081154108047485\n",
      "loss:  0.7468530535697937\n",
      "loss:  0.6706812977790833\n",
      "loss:  0.5024598240852356\n",
      "loss:  0.7130274176597595\n",
      "loss:  0.5696125030517578\n",
      "loss:  0.5645008087158203\n",
      "loss:  0.73458331823349\n",
      "loss:  0.6258452534675598\n",
      "loss:  0.7293068766593933\n",
      "loss:  0.6060714721679688\n",
      "loss:  0.6070464849472046\n",
      "loss:  0.6566579937934875\n",
      "loss:  0.5163137316703796\n",
      "loss:  0.6418192982673645\n",
      "loss:  0.6812914609909058\n",
      "loss:  0.495333194732666\n",
      "loss:  0.5722185969352722\n",
      "loss:  0.7115297317504883\n",
      "loss:  0.614190936088562\n",
      "loss:  0.5050469040870667\n",
      "loss:  0.7276025414466858\n",
      "loss:  0.7603866457939148\n",
      "loss:  0.7090305685997009\n",
      "loss:  0.5254951119422913\n",
      "loss:  0.6464600563049316\n",
      "loss:  0.5174902677536011\n",
      "loss:  0.5079051852226257\n",
      "loss:  0.696264386177063\n",
      "loss:  0.5606123805046082\n",
      "loss:  0.769433856010437\n",
      "loss:  0.6276819705963135\n",
      "loss:  0.7168374061584473\n",
      "loss:  0.7820558547973633\n",
      "loss:  0.5904480218887329\n",
      "loss:  0.44921422004699707\n",
      "loss:  0.6593362092971802\n",
      "loss:  0.5995587110519409\n",
      "loss:  0.5782966613769531\n",
      "loss:  0.7548577785491943\n",
      "loss:  0.6132887601852417\n",
      "loss:  0.5991279482841492\n",
      "loss:  0.5119374394416809\n",
      "loss:  0.5507332682609558\n",
      "loss:  0.5338398218154907\n",
      "loss:  0.6556399464607239\n",
      "loss:  0.6025050282478333\n",
      "loss:  0.6609765291213989\n",
      "loss:  0.7229835391044617\n",
      "loss:  0.5630035400390625\n",
      "loss:  0.5290287137031555\n",
      "loss:  0.5520289540290833\n",
      "loss:  0.6102057099342346\n",
      "loss:  0.5460510849952698\n",
      "loss:  0.5581070184707642\n",
      "loss:  0.5317166447639465\n",
      "loss:  0.6837881803512573\n",
      "loss:  0.5187210440635681\n",
      "loss:  0.6348785161972046\n",
      "loss:  0.6296671628952026\n",
      "loss:  0.6890808343887329\n",
      "loss:  0.48652225732803345\n",
      "loss:  0.7360795140266418\n",
      "loss:  0.7550460696220398\n",
      "loss:  0.495527058839798\n",
      "loss:  0.6880593299865723\n",
      "loss:  0.6408399343490601\n",
      "loss:  0.6172248721122742\n",
      "loss:  0.5511316657066345\n",
      "loss:  0.6065012812614441\n",
      "loss:  0.6369134187698364\n",
      "loss:  0.6148615479469299\n",
      "loss:  0.4670390784740448\n",
      "loss:  0.6885730624198914\n",
      "loss:  0.6004071831703186\n",
      "loss:  0.7450223565101624\n",
      "loss:  0.5970593690872192\n",
      "loss:  0.5223820805549622\n",
      "loss:  0.6784046292304993\n",
      "loss:  0.5031756162643433\n",
      "loss:  0.6732907891273499\n",
      "loss:  0.6385395526885986\n",
      "loss:  0.5936902165412903\n",
      "loss:  0.6063675880432129\n",
      "loss:  0.5135718584060669\n",
      "loss:  0.6334855556488037\n",
      "loss:  0.43538451194763184\n",
      "loss:  0.5751059055328369\n",
      "loss:  0.6911143660545349\n",
      "loss:  0.5387831926345825\n",
      "loss:  0.43399789929389954\n",
      "loss:  0.43224501609802246\n",
      "loss:  0.5921730995178223\n",
      "loss:  0.6325514316558838\n",
      "loss:  0.4953218400478363\n",
      "loss:  0.7555756568908691\n",
      "loss:  0.45629259943962097\n",
      "loss:  0.6630752086639404\n",
      "loss:  0.6492969989776611\n",
      "loss:  0.5205796957015991\n",
      "loss:  0.7520904541015625\n",
      "loss:  0.7675877213478088\n",
      "loss:  0.6063221096992493\n",
      "loss:  0.6040230989456177\n",
      "loss:  0.6801585555076599\n",
      "loss:  0.4077589511871338\n",
      "loss:  0.6004731059074402\n",
      "loss:  0.6175726652145386\n",
      "loss:  0.5358167290687561\n",
      "loss:  0.593794584274292\n",
      "loss:  0.5498150587081909\n",
      "loss:  0.5502735376358032\n",
      "loss:  0.5638413429260254\n",
      "loss:  0.6972444653511047\n",
      "loss:  0.5776230096817017\n",
      "loss:  0.45914947986602783\n",
      "loss:  0.6157739162445068\n",
      "loss:  0.5612576007843018\n",
      "loss:  0.7216206789016724\n",
      "loss:  0.5604695677757263\n",
      "loss:  0.5799411535263062\n",
      "loss:  0.6438442468643188\n",
      "loss:  0.5959518551826477\n",
      "loss:  0.47529834508895874\n",
      "loss:  0.6316013336181641\n",
      "loss:  0.6741486191749573\n",
      "loss:  0.6725616455078125\n",
      "loss:  0.5874804854393005\n",
      "loss:  0.7308945059776306\n",
      "loss:  0.5889343619346619\n",
      "loss:  0.8180629014968872\n",
      "loss:  0.5870828628540039\n",
      "loss:  0.5300517678260803\n",
      "loss:  0.6415265798568726\n",
      "loss:  0.47872117161750793\n",
      "loss:  0.5166166424751282\n",
      "loss:  0.5598254799842834\n",
      "loss:  0.5144328474998474\n",
      "loss:  0.6613587737083435\n",
      "loss:  0.6479443907737732\n",
      "loss:  0.6121540665626526\n",
      "loss:  0.5306488275527954\n",
      "loss:  0.827086329460144\n",
      "loss:  0.5016332864761353\n",
      "loss:  0.6636714339256287\n",
      "loss:  0.6236620545387268\n",
      "loss:  0.5054143667221069\n",
      "loss:  0.5459015369415283\n",
      "loss:  0.6315311193466187\n",
      "loss:  0.6243484616279602\n",
      "loss:  0.6881455183029175\n",
      "loss:  0.8037907481193542\n",
      "loss:  0.7168762683868408\n",
      "loss:  0.627129077911377\n",
      "loss:  0.6696505546569824\n",
      "loss:  0.5058867335319519\n",
      "loss:  0.41752728819847107\n",
      "loss:  0.5886866450309753\n",
      "loss:  0.6334447860717773\n",
      "loss:  0.5425823330879211\n",
      "loss:  0.5065628886222839\n",
      "loss:  0.5568792819976807\n",
      "loss:  0.5126447081565857\n",
      "loss:  0.523771345615387\n",
      "loss:  0.5565704107284546\n",
      "loss:  0.5562613010406494\n",
      "loss:  0.6075366735458374\n",
      "loss:  0.5091144442558289\n",
      "loss:  0.5727955102920532\n",
      "loss:  0.5885100364685059\n",
      "loss:  0.6348140239715576\n",
      "loss:  0.5993186235427856\n",
      "loss:  0.5741066336631775\n",
      "loss:  0.5066503882408142\n",
      "loss:  0.6086769104003906\n",
      "loss:  0.5693653225898743\n",
      "loss:  0.6274840831756592\n",
      "loss:  0.7511280179023743\n",
      "loss:  0.7175600528717041\n",
      "loss:  0.648446798324585\n",
      "loss:  0.5333575010299683\n",
      "loss:  0.5141311287879944\n",
      "loss:  0.48566070199012756\n",
      "loss:  0.6003606915473938\n",
      "loss:  0.6237050890922546\n",
      "loss:  0.5014187097549438\n",
      "loss:  0.49140283465385437\n",
      "loss:  0.43285754323005676\n",
      "loss:  0.6260812282562256\n",
      "loss:  0.4832388460636139\n",
      "loss:  0.6736128330230713\n",
      "loss:  0.7961347699165344\n",
      "loss:  0.5718427896499634\n",
      "loss:  0.627996027469635\n",
      "loss:  0.6090577244758606\n",
      "loss:  0.6155771017074585\n",
      "loss:  0.5409528017044067\n",
      "loss:  0.3855000138282776\n",
      "loss:  0.630770206451416\n",
      "loss:  0.42248404026031494\n",
      "loss:  0.5253791213035583\n",
      "loss:  0.5864456295967102\n",
      "loss:  0.5158273577690125\n",
      "loss:  0.5772187113761902\n",
      "loss:  0.6323490142822266\n",
      "loss:  0.5619411468505859\n",
      "loss:  0.6724960803985596\n",
      "loss:  0.6347445249557495\n",
      "loss:  0.579430878162384\n",
      "loss:  0.47120943665504456\n",
      "loss:  0.5803420543670654\n",
      "loss:  0.5198285579681396\n",
      "loss:  0.5727666020393372\n",
      "loss:  0.6534004807472229\n",
      "loss:  0.5820724368095398\n",
      "loss:  0.5880988836288452\n",
      "loss:  0.8100836873054504\n",
      "loss:  0.5454912781715393\n",
      "loss:  0.5934704542160034\n",
      "loss:  0.6019901633262634\n",
      "loss:  0.5036239624023438\n",
      "loss:  0.7096956372261047\n",
      "loss:  0.6055972576141357\n",
      "loss:  0.4316999912261963\n",
      "loss:  0.6423259377479553\n",
      "loss:  0.4873887002468109\n",
      "loss:  0.5965763330459595\n",
      "loss:  0.7184087634086609\n",
      "loss:  0.5171647667884827\n",
      "loss:  0.6482078433036804\n",
      "loss:  0.5060481429100037\n",
      "loss:  0.5170895457267761\n",
      "loss:  0.7522677183151245\n",
      "loss:  0.5588564276695251\n",
      "loss:  0.6959521770477295\n",
      "loss:  0.6516236662864685\n",
      "loss:  0.5530641078948975\n",
      "loss:  0.4902742803096771\n",
      "loss:  0.5795629024505615\n",
      "loss:  0.5805543661117554\n",
      "loss:  0.5299791693687439\n",
      "loss:  0.6511332392692566\n",
      "loss:  0.6946276426315308\n",
      "loss:  0.5567037463188171\n",
      "loss:  0.5538370609283447\n",
      "loss:  0.5425798296928406\n",
      "loss:  0.5866297483444214\n",
      "loss:  0.6731529831886292\n",
      "loss:  0.5336155295372009\n",
      "loss:  0.5882640480995178\n",
      "loss:  0.39021971821784973\n",
      "loss:  0.5892645120620728\n",
      "loss:  0.5548334121704102\n",
      "loss:  0.6218286156654358\n",
      "loss:  0.658919632434845\n",
      "loss:  0.5968760251998901\n",
      "loss:  0.5759852528572083\n",
      "loss:  0.5962850451469421\n",
      "loss:  0.41603052616119385\n",
      "loss:  0.7643925547599792\n",
      "loss:  0.4961428940296173\n",
      "loss:  0.580051839351654\n",
      "loss:  0.530161440372467\n",
      "loss:  0.7128264307975769\n",
      "loss:  0.6029919385910034\n",
      "loss:  0.4361388087272644\n",
      "loss:  0.5589752197265625\n",
      "loss:  0.5918764472007751\n",
      "loss:  0.6608230471611023\n",
      "loss:  0.5601435303688049\n",
      "loss:  0.6187282204627991\n",
      "loss:  0.6185283660888672\n",
      "loss:  0.5111607909202576\n",
      "loss:  0.5688067674636841\n",
      "loss:  0.5029119253158569\n",
      "loss:  0.7510251998901367\n",
      "loss:  0.4070242643356323\n",
      "loss:  0.6315954327583313\n",
      "loss:  0.5939105749130249\n",
      "loss:  0.7910171151161194\n",
      "loss:  0.5064429044723511\n",
      "loss:  0.6015917062759399\n",
      "loss:  0.5287127494812012\n",
      "loss:  0.5491941571235657\n",
      "loss:  0.6587436199188232\n",
      "loss:  0.41416287422180176\n",
      "loss:  0.5491900444030762\n",
      "loss:  0.5488778948783875\n",
      "loss:  0.6340701580047607\n",
      "loss:  0.6641523241996765\n",
      "loss:  0.6583645939826965\n",
      "loss:  0.6446340084075928\n",
      "loss:  0.6131622791290283\n",
      "loss:  0.60225510597229\n",
      "loss:  0.6571425795555115\n",
      "loss:  0.8278146982192993\n",
      "loss:  0.5291013121604919\n",
      "loss:  0.5244150161743164\n",
      "loss:  0.6011049151420593\n",
      "loss:  0.45034143328666687\n",
      "loss:  0.6357679963111877\n",
      "loss:  0.5433741807937622\n",
      "loss:  0.4579579830169678\n",
      "loss:  0.5484893918037415\n",
      "loss:  0.48710981011390686\n",
      "loss:  0.5272809863090515\n",
      "loss:  0.5900416374206543\n",
      "loss:  0.781783938407898\n",
      "loss:  0.5092194676399231\n",
      "loss:  0.5777186751365662\n",
      "loss:  0.4976853132247925\n",
      "loss:  0.5992090106010437\n",
      "loss:  0.6224193572998047\n",
      "loss:  0.7022653818130493\n",
      "loss:  0.6724711656570435\n",
      "loss:  0.47152453660964966\n",
      "loss:  0.4542742967605591\n",
      "loss:  0.5996800065040588\n",
      "loss:  0.4780377149581909\n",
      "loss:  0.5021184682846069\n",
      "loss:  0.5580477714538574\n",
      "loss:  0.6491469740867615\n",
      "loss:  0.5932919979095459\n",
      "loss:  0.7200390100479126\n",
      "loss:  0.5333173871040344\n",
      "loss:  0.5368157625198364\n",
      "loss:  0.5759159326553345\n",
      "loss:  0.5717945694923401\n",
      "loss:  0.43663498759269714\n",
      "loss:  0.5629395842552185\n",
      "loss:  0.5790933966636658\n",
      "loss:  0.4500851035118103\n",
      "loss:  0.6467624306678772\n",
      "loss:  0.6137231588363647\n",
      "loss:  0.3106595575809479\n",
      "loss:  0.6951686143875122\n",
      "loss:  0.7292405962944031\n",
      "loss:  0.6534273028373718\n",
      "loss:  0.6017003655433655\n",
      "loss:  0.5580374002456665\n",
      "loss:  0.7146319150924683\n",
      "loss:  0.6640625\n",
      "loss:  0.5456283092498779\n",
      "loss:  0.6246718764305115\n",
      "loss:  0.4947132170200348\n",
      "loss:  0.5735489726066589\n",
      "loss:  0.628818929195404\n",
      "loss:  0.6207212209701538\n",
      "loss:  0.6059015989303589\n",
      "loss:  0.5925907492637634\n",
      "loss:  0.5015870332717896\n",
      "loss:  0.5306060910224915\n",
      "loss:  0.6308899521827698\n",
      "loss:  0.7705481648445129\n",
      "loss:  0.5242246389389038\n",
      "loss:  0.5602789521217346\n",
      "loss:  0.6713710427284241\n",
      "loss:  0.6293509602546692\n",
      "loss:  0.5902469754219055\n",
      "loss:  0.7894866466522217\n",
      "loss:  0.6467493176460266\n",
      "loss:  0.6900860667228699\n",
      "loss:  0.717798113822937\n",
      "loss:  0.6112937331199646\n",
      "loss:  0.5863112211227417\n",
      "loss:  0.6096438765525818\n",
      "loss:  0.4414687156677246\n",
      "loss:  0.577415406703949\n",
      "loss:  0.6646728515625\n",
      "loss:  0.6242513656616211\n",
      "loss:  0.7084779143333435\n",
      "loss:  0.5478156805038452\n",
      "loss:  0.6177307367324829\n",
      "loss:  0.5508024096488953\n",
      "loss:  0.47993865609169006\n",
      "loss:  0.6081851720809937\n",
      "loss:  0.6808424592018127\n",
      "loss:  0.6700348854064941\n",
      "loss:  0.48222416639328003\n",
      "loss:  0.6062018871307373\n",
      "loss:  0.6429034471511841\n",
      "loss:  0.6687782406806946\n",
      "loss:  0.3566916882991791\n",
      "loss:  0.7080686688423157\n",
      "loss:  0.6263472437858582\n",
      "loss:  0.46760672330856323\n",
      "loss:  0.7163354158401489\n",
      "loss:  0.5726026892662048\n",
      "loss:  0.49815475940704346\n",
      "loss:  0.4987158477306366\n",
      "loss:  0.5632269382476807\n",
      "loss:  0.5577545762062073\n",
      "loss:  0.5231579542160034\n",
      "loss:  0.700240969657898\n",
      "loss:  0.678175687789917\n",
      "loss:  0.6534941792488098\n",
      "loss:  0.594254195690155\n",
      "loss:  0.5945857167243958\n",
      "loss:  0.6286314725875854\n",
      "loss:  0.526697039604187\n",
      "loss:  0.6149100065231323\n",
      "loss:  0.5813818573951721\n",
      "loss:  0.4734589755535126\n",
      "loss:  0.6079531311988831\n",
      "loss:  0.6058310866355896\n",
      "loss:  0.680228590965271\n",
      "loss:  0.40080326795578003\n",
      "loss:  0.49251800775527954\n",
      "loss:  0.625942587852478\n",
      "loss:  0.6471894979476929\n",
      "loss:  0.48522141575813293\n",
      "loss:  0.528682291507721\n",
      "loss:  0.6423125267028809\n",
      "loss:  0.3809274435043335\n",
      "loss:  0.6324832439422607\n",
      "loss:  0.5463882684707642\n",
      "loss:  0.6340901851654053\n",
      "loss:  0.6415508985519409\n",
      "loss:  0.5975064039230347\n",
      "loss:  0.6095141172409058\n",
      "loss:  0.5466635823249817\n",
      "loss:  0.5319145917892456\n",
      "loss:  0.5916845798492432\n",
      "loss:  0.5824090838432312\n",
      "loss:  0.5302824378013611\n",
      "loss:  0.5739436745643616\n",
      "loss:  0.656779944896698\n",
      "loss:  0.6044372916221619\n",
      "loss:  0.7632619142532349\n",
      "loss:  0.4170176684856415\n",
      "loss:  0.5697433352470398\n",
      "loss:  0.5807226300239563\n",
      "loss:  0.6289024949073792\n",
      "loss:  0.4832625985145569\n",
      "loss:  0.5794510245323181\n",
      "loss:  0.517694354057312\n",
      "loss:  0.5876747965812683\n",
      "loss:  0.5746644735336304\n",
      "loss:  0.7003805041313171\n",
      "loss:  0.5854939222335815\n",
      "loss:  0.5803706645965576\n",
      "loss:  0.6668247580528259\n",
      "loss:  0.5629718899726868\n",
      "loss:  0.6500295996665955\n",
      "loss:  0.6715750098228455\n",
      "loss:  0.6180967688560486\n",
      "loss:  0.6148226857185364\n",
      "loss:  0.5058083534240723\n",
      "loss:  0.5182651877403259\n",
      "loss:  0.584187924861908\n",
      "loss:  0.47521060705184937\n",
      "loss:  0.5787781476974487\n",
      "loss:  0.6742874979972839\n",
      "loss:  0.5949084758758545\n",
      "loss:  0.44891121983528137\n",
      "loss:  0.5942673683166504\n",
      "loss:  0.5897620916366577\n",
      "loss:  0.5083187222480774\n",
      "loss:  0.6988462805747986\n",
      "loss:  0.5702420473098755\n",
      "loss:  0.6434281468391418\n",
      "loss:  0.5854716897010803\n",
      "loss:  0.6279394030570984\n",
      "loss:  0.5314498543739319\n",
      "loss:  0.4809724688529968\n",
      "loss:  0.6686984896659851\n",
      "loss:  0.4112533628940582\n",
      "loss:  0.5499712824821472\n",
      "loss:  0.5066760778427124\n",
      "loss:  0.620826244354248\n",
      "loss:  0.4771001636981964\n",
      "loss:  0.6046421527862549\n",
      "loss:  0.7044869661331177\n",
      "loss:  0.5224832892417908\n",
      "loss:  0.789914071559906\n",
      "loss:  0.6806578040122986\n",
      "loss:  0.6445419192314148\n",
      "loss:  0.41397416591644287\n",
      "loss:  0.7648196220397949\n",
      "loss:  0.533364474773407\n",
      "loss:  0.6281223893165588\n",
      "loss:  0.554867684841156\n",
      "loss:  0.569330096244812\n",
      "loss:  0.5745499730110168\n",
      "loss:  0.5249384641647339\n",
      "loss:  0.6389915943145752\n",
      "loss:  0.624828577041626\n",
      "loss:  0.49697160720825195\n",
      "loss:  0.6263054609298706\n",
      "loss:  0.5955452919006348\n",
      "loss:  0.6208900809288025\n",
      "loss:  0.4558551609516144\n",
      "loss:  0.725440263748169\n",
      "loss:  0.7521691918373108\n",
      "loss:  0.7168906331062317\n",
      "loss:  0.777350127696991\n",
      "loss:  0.5537471771240234\n",
      "loss:  0.3760513365268707\n",
      "loss:  0.5174500942230225\n",
      "loss:  0.5424738526344299\n",
      "loss:  0.42927271127700806\n",
      "loss:  0.4960377514362335\n",
      "loss:  0.6301195621490479\n",
      "loss:  0.5794095396995544\n",
      "loss:  0.6586177349090576\n",
      "loss:  0.5301976203918457\n",
      "loss:  0.4375455379486084\n",
      "loss:  0.4474341869354248\n",
      "loss:  0.5964374542236328\n",
      "loss:  0.48820263147354126\n",
      "loss:  0.6853940486907959\n",
      "loss:  0.5794329047203064\n",
      "loss:  0.5898267030715942\n",
      "loss:  0.5829544067382812\n",
      "loss:  0.4125674068927765\n",
      "loss:  0.5339542627334595\n",
      "loss:  0.5428540706634521\n",
      "loss:  0.5843473076820374\n",
      "loss:  0.5475723743438721\n",
      "loss:  0.46839988231658936\n",
      "loss:  0.7027398347854614\n",
      "loss:  0.6118190288543701\n",
      "loss:  0.657028079032898\n",
      "loss:  0.6043590307235718\n",
      "loss:  0.6473912596702576\n",
      "loss:  0.6882163882255554\n",
      "loss:  0.7318184971809387\n",
      "loss:  0.43490901589393616\n",
      "loss:  0.505388081073761\n",
      "loss:  0.6328461170196533\n",
      "loss:  0.5486243963241577\n",
      "loss:  0.5733507871627808\n",
      "loss:  0.6524552702903748\n",
      "loss:  0.5810577273368835\n",
      "loss:  0.4487766623497009\n",
      "loss:  0.576176106929779\n",
      "loss:  0.5202885866165161\n",
      "loss:  0.584324836730957\n",
      "loss:  0.6306153535842896\n",
      "loss:  0.6098699569702148\n",
      "loss:  0.5764428973197937\n",
      "loss:  0.43271738290786743\n",
      "loss:  0.4797949492931366\n",
      "loss:  0.4245547652244568\n",
      "loss:  0.4240788221359253\n",
      "loss:  0.6004500985145569\n",
      "loss:  0.6915992498397827\n",
      "loss:  0.5099270343780518\n",
      "loss:  0.6505200862884521\n",
      "loss:  0.4780373275279999\n",
      "loss:  0.5499226450920105\n",
      "loss:  0.4994073212146759\n",
      "loss:  0.5550709366798401\n",
      "loss:  0.596139669418335\n",
      "loss:  0.4943290054798126\n",
      "loss:  0.552940309047699\n",
      "loss:  0.4876348376274109\n",
      "loss:  0.6422470808029175\n",
      "loss:  0.5840808153152466\n",
      "loss:  0.5393742918968201\n",
      "loss:  0.5388720035552979\n",
      "loss:  0.5545814037322998\n",
      "loss:  0.4988962411880493\n",
      "loss:  0.47631967067718506\n",
      "loss:  0.4548187851905823\n",
      "loss:  0.49002400040626526\n",
      "loss:  0.687389612197876\n",
      "loss:  0.575604259967804\n",
      "loss:  0.6112781763076782\n",
      "loss:  0.5635538101196289\n",
      "loss:  0.6895586848258972\n",
      "loss:  0.5231528878211975\n",
      "loss:  0.5037246942520142\n",
      "loss:  0.5151369571685791\n",
      "loss:  0.7367587089538574\n",
      "loss:  0.526627779006958\n",
      "loss:  0.7114897966384888\n",
      "loss:  0.5760834813117981\n",
      "loss:  0.5953829288482666\n",
      "loss:  0.4155748784542084\n",
      "loss:  0.5823991298675537\n",
      "loss:  0.6604565978050232\n",
      "loss:  0.6290333271026611\n",
      "loss:  0.6666998267173767\n",
      "loss:  0.46067172288894653\n",
      "loss:  0.5627477169036865\n",
      "loss:  0.4449644088745117\n",
      "loss:  0.41416653990745544\n",
      "loss:  0.6756582856178284\n",
      "loss:  0.52791827917099\n",
      "loss:  0.4507477581501007\n",
      "loss:  0.615268886089325\n",
      "loss:  0.48218896985054016\n",
      "loss:  0.697790801525116\n",
      "loss:  0.4344092309474945\n",
      "loss:  0.5922303199768066\n",
      "loss:  0.6414783596992493\n",
      "loss:  0.6586781144142151\n",
      "loss:  0.6267006397247314\n",
      "loss:  0.5395662784576416\n",
      "loss:  0.4458591938018799\n",
      "loss:  0.4168168306350708\n",
      "loss:  0.6248289346694946\n",
      "loss:  0.5994297862052917\n",
      "loss:  0.5088598132133484\n",
      "loss:  0.6295770406723022\n",
      "loss:  0.5334217548370361\n",
      "loss:  0.6346895694732666\n",
      "loss:  0.623019814491272\n",
      "loss:  0.3885088264942169\n",
      "loss:  0.6173773407936096\n",
      "loss:  0.5687360167503357\n",
      "loss:  0.6045526266098022\n",
      "loss:  0.48103174567222595\n",
      "loss:  0.46161332726478577\n",
      "loss:  0.6733042597770691\n",
      "loss:  0.5534761548042297\n",
      "loss:  0.629077672958374\n",
      "loss:  0.6127220988273621\n",
      "loss:  0.5900797247886658\n",
      "loss:  0.597920298576355\n",
      "loss:  0.563011109828949\n",
      "loss:  0.70744389295578\n",
      "loss:  0.5289592146873474\n",
      "loss:  0.6640895009040833\n",
      "loss:  0.550401508808136\n",
      "loss:  0.6375137567520142\n",
      "loss:  0.55375736951828\n",
      "loss:  0.38685211539268494\n",
      "loss:  0.5539059042930603\n",
      "loss:  0.5436016321182251\n",
      "loss:  0.6328474879264832\n",
      "loss:  0.48721009492874146\n",
      "loss:  0.5969791412353516\n",
      "loss:  0.6123946309089661\n",
      "loss:  0.4854510724544525\n",
      "loss:  0.6353307366371155\n",
      "loss:  0.6131165623664856\n",
      "loss:  0.4889836609363556\n",
      "loss:  0.593795657157898\n",
      "loss:  0.5894244313240051\n",
      "loss:  0.5549612641334534\n",
      "loss:  0.5081274509429932\n",
      "loss:  0.6327183842658997\n",
      "loss:  0.5794929265975952\n",
      "loss:  0.5549608469009399\n",
      "loss:  0.6386660933494568\n",
      "loss:  0.559870719909668\n",
      "loss:  0.6307452917098999\n",
      "loss:  0.5458152294158936\n",
      "loss:  0.5565531253814697\n",
      "loss:  0.5186907649040222\n",
      "loss:  0.5422173738479614\n",
      "loss:  0.619586706161499\n",
      "loss:  0.5219182968139648\n",
      "loss:  0.6412324905395508\n",
      "loss:  0.5856947898864746\n",
      "loss:  0.47054433822631836\n",
      "loss:  0.4421273171901703\n",
      "loss:  0.513594925403595\n",
      "loss:  0.6484348773956299\n",
      "loss:  0.70280921459198\n",
      "loss:  0.5859967470169067\n",
      "loss:  0.7104146480560303\n",
      "loss:  0.4959349036216736\n",
      "loss:  0.508756160736084\n",
      "loss:  0.4767782688140869\n",
      "loss:  0.5538111925125122\n",
      "loss:  0.5262998342514038\n",
      "loss:  0.7156890630722046\n",
      "loss:  0.6449417471885681\n",
      "loss:  0.5330523252487183\n",
      "loss:  0.4659002423286438\n",
      "loss:  0.5894781351089478\n",
      "loss:  0.7479089498519897\n",
      "loss:  0.5089274644851685\n",
      "loss:  0.5700545907020569\n",
      "loss:  0.5461028218269348\n",
      "loss:  0.5081113576889038\n",
      "loss:  0.5669808983802795\n",
      "loss:  0.5410583019256592\n",
      "loss:  0.6658257842063904\n",
      "loss:  0.4410068094730377\n",
      "loss:  0.5277747511863708\n",
      "loss:  0.4417118430137634\n",
      "loss:  0.5074970126152039\n",
      "loss:  0.5322897434234619\n",
      "loss:  0.5181174278259277\n",
      "loss:  0.41554275155067444\n",
      "loss:  0.4530752897262573\n",
      "loss:  0.655170738697052\n",
      "loss:  0.6572473645210266\n",
      "loss:  0.5081669092178345\n",
      "loss:  0.5471308827400208\n",
      "loss:  0.49147793650627136\n",
      "loss:  0.572064995765686\n",
      "loss:  0.6554844975471497\n",
      "loss:  0.6970745921134949\n",
      "loss:  0.4942500591278076\n",
      "loss:  0.5105932354927063\n",
      "loss:  0.46849337220191956\n",
      "loss:  0.4145728647708893\n",
      "loss:  0.5978621244430542\n",
      "loss:  0.46712726354599\n",
      "loss:  0.5811354517936707\n",
      "loss:  0.41428402066230774\n",
      "loss:  0.5921117663383484\n",
      "loss:  0.7057954668998718\n",
      "loss:  0.7381632328033447\n",
      "loss:  0.4975566565990448\n",
      "loss:  0.700441837310791\n",
      "loss:  0.6910277605056763\n",
      "loss:  0.6087857484817505\n",
      "loss:  0.4392910301685333\n",
      "loss:  0.5893349647521973\n",
      "loss:  0.5535512566566467\n",
      "loss:  0.6678590178489685\n",
      "loss:  0.5640463829040527\n",
      "loss:  0.6020843386650085\n",
      "loss:  0.6224658489227295\n",
      "loss:  0.500247061252594\n",
      "loss:  0.632646918296814\n",
      "loss:  0.7302151918411255\n",
      "loss:  0.5872918367385864\n",
      "loss:  0.5707072019577026\n",
      "loss:  0.4845547378063202\n",
      "loss:  0.6632294654846191\n",
      "loss:  0.5886384844779968\n",
      "loss:  0.6107167601585388\n",
      "loss:  0.5472919344902039\n",
      "loss:  0.5808981657028198\n",
      "loss:  0.6080238223075867\n",
      "loss:  0.6366355419158936\n",
      "loss:  0.5164123177528381\n",
      "loss:  0.6936620473861694\n",
      "loss:  0.6627042889595032\n",
      "loss:  0.5187963843345642\n",
      "loss:  0.7157321572303772\n",
      "loss:  0.4946272373199463\n",
      "loss:  0.6280248165130615\n",
      "loss:  0.5508553981781006\n",
      "loss:  0.6069573760032654\n",
      "loss:  0.47371360659599304\n",
      "loss:  0.5147591233253479\n",
      "loss:  0.4118165373802185\n",
      "loss:  0.6152105331420898\n",
      "loss:  0.5204845666885376\n",
      "loss:  0.5525228381156921\n",
      "loss:  0.583658754825592\n",
      "loss:  0.5161875486373901\n",
      "loss:  0.5898051857948303\n",
      "loss:  0.5160699486732483\n",
      "loss:  0.6479975581169128\n",
      "loss:  0.5958974957466125\n",
      "loss:  0.5046086311340332\n",
      "loss:  0.4696694314479828\n",
      "loss:  0.5831472873687744\n",
      "loss:  0.5401061773300171\n",
      "loss:  0.5195415019989014\n",
      "loss:  0.47821515798568726\n",
      "loss:  0.49812251329421997\n",
      "loss:  0.4138844609260559\n",
      "loss:  0.6822609305381775\n",
      "loss:  0.5966531038284302\n",
      "loss:  0.49916011095046997\n",
      "loss:  0.5686825513839722\n",
      "loss:  0.6461510062217712\n",
      "loss:  0.5615159869194031\n",
      "loss:  0.5265914797782898\n",
      "loss:  0.6325411796569824\n",
      "loss:  0.3892706334590912\n",
      "loss:  0.5432711243629456\n",
      "loss:  0.6575105786323547\n",
      "loss:  0.5535038113594055\n",
      "loss:  0.5473901629447937\n",
      "loss:  0.5618244409561157\n",
      "loss:  0.49117469787597656\n",
      "loss:  0.46905088424682617\n",
      "loss:  0.4989739656448364\n",
      "loss:  0.5068795680999756\n",
      "loss:  0.5444890856742859\n",
      "loss:  0.46776577830314636\n",
      "loss:  0.6432862281799316\n",
      "loss:  0.5168249011039734\n",
      "loss:  0.36775946617126465\n",
      "loss:  0.578444242477417\n",
      "loss:  0.5624381899833679\n",
      "loss:  0.44675782322883606\n",
      "loss:  0.5978873372077942\n",
      "loss:  0.4196835160255432\n",
      "loss:  0.6892687082290649\n",
      "loss:  0.6783713102340698\n",
      "loss:  0.4667993485927582\n",
      "loss:  0.5834463834762573\n",
      "loss:  0.5620420575141907\n",
      "loss:  0.6270259022712708\n",
      "loss:  0.43702182173728943\n",
      "loss:  0.6185760498046875\n",
      "loss:  0.5210002660751343\n",
      "loss:  0.41113045811653137\n",
      "loss:  0.5502621531486511\n",
      "loss:  0.5625613927841187\n",
      "loss:  0.5965883135795593\n",
      "loss:  0.5915951728820801\n",
      "loss:  0.4824419617652893\n",
      "loss:  0.6715772151947021\n",
      "loss:  0.5946896076202393\n",
      "loss:  0.5940316319465637\n",
      "loss:  0.6214701533317566\n",
      "loss:  0.42006635665893555\n",
      "loss:  0.5379490852355957\n",
      "loss:  0.6926938891410828\n",
      "loss:  0.5470150113105774\n",
      "loss:  0.640891432762146\n",
      "loss:  0.4414803385734558\n",
      "loss:  0.6418349146842957\n",
      "loss:  0.449505478143692\n",
      "loss:  0.5890293121337891\n",
      "loss:  0.5931187868118286\n",
      "loss:  0.7107676863670349\n",
      "loss:  0.6109943985939026\n",
      "loss:  0.6427584290504456\n",
      "loss:  0.5087274312973022\n",
      "loss:  0.3982801139354706\n",
      "loss:  0.6189497113227844\n",
      "loss:  0.675338089466095\n",
      "loss:  0.5097722411155701\n",
      "loss:  0.4258434772491455\n",
      "loss:  0.5035886168479919\n",
      "loss:  0.5678620338439941\n",
      "loss:  0.637442946434021\n",
      "loss:  0.5238937735557556\n",
      "loss:  0.6430416107177734\n",
      "loss:  0.6750973463058472\n",
      "loss:  0.7013514637947083\n",
      "loss:  0.6415911912918091\n",
      "loss:  0.4476371705532074\n",
      "loss:  0.5207839608192444\n",
      "loss:  0.5735604166984558\n",
      "loss:  0.5759003162384033\n",
      "loss:  0.715110182762146\n",
      "loss:  0.4198545813560486\n",
      "loss:  0.5000289678573608\n",
      "loss:  0.6754317283630371\n",
      "loss:  0.6134562492370605\n",
      "loss:  0.5261297225952148\n",
      "loss:  0.3820926547050476\n",
      "loss:  0.5332045555114746\n",
      "loss:  0.5159274339675903\n",
      "loss:  0.7378991842269897\n",
      "loss:  0.47887933254241943\n",
      "loss:  0.4485202431678772\n",
      "loss:  0.5519419312477112\n",
      "loss:  0.4993387758731842\n",
      "loss:  0.5002615451812744\n",
      "loss:  0.5586771368980408\n",
      "loss:  0.4864961504936218\n",
      "loss:  0.777740478515625\n",
      "loss:  0.5679740309715271\n",
      "loss:  0.6535260677337646\n",
      "loss:  0.4548276364803314\n",
      "loss:  0.5996575355529785\n",
      "loss:  0.48785945773124695\n",
      "loss:  0.6402081251144409\n",
      "loss:  0.5594193935394287\n",
      "loss:  0.5287063717842102\n",
      "loss:  0.5383556485176086\n",
      "loss:  0.6906524300575256\n",
      "loss:  0.5518901944160461\n",
      "loss:  0.4623326361179352\n",
      "loss:  0.5938655138015747\n",
      "loss:  0.5696691274642944\n",
      "loss:  0.7178819179534912\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch in dl:\n",
    "        # retrieve one batch of data from the dataloader\n",
    "        inputs, targets = batch['image'].squeeze(dim=1), batch['label']\n",
    "\n",
    "        # before anything else, make sure we have no gradients yet calculated/saved,\n",
    "        # for any of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the output of the model\n",
    "        output = model(inputs)\n",
    "\n",
    "        # calculate the loss (error) of the model with respect to the image labels (targets)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        # print and save this loss value\n",
    "        print('loss: ', loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # this is where the magic happens; PyTorch now calculates the gradients AUTOMATICALLY for us,\n",
    "        # and the optimizer updates the parameters with these gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527ea62-7d4c-4945-9fab-8f38a0ddc02d",
   "metadata": {},
   "source": [
    "## 3. Visualizing the loss curve\n",
    "\n",
    "And voila! Hopefully you notice the loss going down as the model sees more and more images, and \"learns\" to calibrate on the labels of the images.\n",
    "\n",
    "We can visualize this loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814f2fcb-a49d-4891-bcba-61633a78c5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3b6c325590>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9d186-b954-4d70-a8cf-54f4ac497231",
   "metadata": {},
   "source": [
    "This is our loss curve. Although it's a bit noisy, we can clearly see a trend downwards (minimizing the error/loss)!\n",
    "\n",
    "For example, this is the loss graph of the LLama 1 language models, released by Meta in 2023 (paper: \"LLaMA: Open and Efficient Foundation Language Models\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a275f-2816-43a7-ab31-b8ef5c62d811",
   "metadata": {},
   "source": [
    "![Llama 1 loss curve](./llama1_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0736625-1ad0-42c8-9341-4109d94a8323",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d0fbf-c467-4111-86bf-cae81716637e",
   "metadata": {},
   "source": [
    "Let's see how well the model behaves on the test set, i.e. the set of images that the model has NOT seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b455f4e-18c0-4cc0-abcd-4ab03d634d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 87.81%\n"
     ]
    }
   ],
   "source": [
    "# set the model in evaluation mode; this tells PyTorch \"do not calculate the gradients for me,\n",
    "# as I do not need these now\"\n",
    "model.eval()\n",
    "\n",
    "# let's also create a simple dataloader for the test set; although not necessary, it's easier\n",
    "# to work with one (we'll evaluate one example at a time, and we don't need shuffling)\n",
    "test_dl = torch.utils.data.DataLoader(mnist['test'], batch_size=1)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # iterate over the test set images\n",
    "    for batch in test_dl:\n",
    "        # get the input image and the label\n",
    "        img, target = batch['image'].squeeze(dim=1), batch['label']\n",
    "\n",
    "        # pass the image through the model\n",
    "        output = model(img)\n",
    "\n",
    "        # get the predicted class (the highest probability among the 10 classes of digits)\n",
    "        predicted_class = torch.argmax(output, dim=1)\n",
    "\n",
    "        # if correct, add 1\n",
    "        total += 1\n",
    "        correct += (predicted_class == target).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eba0bf-b380-4e0b-9cdf-9c9403f47973",
   "metadata": {},
   "source": [
    "Great, we obtained an accuracy of 87.85% (at least on my machine, might be slightly different on different runs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91a083-cb3d-46eb-b247-4a7d428d991e",
   "metadata": {},
   "source": [
    "## 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23530a3c-9803-45f4-81e1-56ec291a1eeb",
   "metadata": {},
   "source": [
    "Great, we trained from scratch a neural network to recognize images of digits, with a nice accuracy of 87%! Even though we trained this on CPU, it was still quite fast. If you have a GPU and willing to experiment, you can try to see just how much faster it goes!\n",
    "\n",
    "You can try playing around with a lot of things in this notebook, and try to make it work better. For example, you can try a deeper/wider network architecture, or you can try with a different learning rate, batch size, number of epochs etc. Feel free to experiment, run, re-run and make sure you understand what each bit of the code does.\n",
    "\n",
    "Here's just a simple example of a deeper and wider model, with ~12M parameters. Experiment and see what works best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8bf2f6-5e8f-4b6c-ab05-9e5a85994e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1568, out_features=3136, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=3136, out_features=1568, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=1568, out_features=784, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28, 2 * 28*28),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2 * 28*28, 4 * 28*28),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4 * 28*28, 2 * 28*28),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2 * 28*28, 28*28),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(28*28, 10),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f725da-443e-47b4-803b-cd46b1923b10",
   "metadata": {},
   "source": [
    "Finally, a note about modern state-of-the-art vision models. We've only worked with a simple neural network with nothing more but linearities and non-linearities in the form of ReLU functions. This is very primitive, and people don't usually do this for vision models nowadays. Instead, they use something called CNN's (convolutional neural networks). There's a bunch of intros to CNN's on the internet, so just look one up if you're interested. We'll skip these for now, and work with language models only for this course, using the Transformer architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
